# 1. Introduction - Táº¡i sao chÃºng ta cáº§n GA vÃ  SHAP?

![Mutation](./images/thumb.png)

NgÃ y nay, AI gáº§n nhÆ° Ä‘Ã£ Ä‘Æ°á»£c Ä‘Æ°a vÃ o á»©ng dá»¥ng trong háº§u háº¿t cÃ¡c ngÃ nh nghá», tá»« y táº¿, sinh há»c Ä‘áº¿n ká»¹ thuáº­t sáº£n xuáº¥t cÃ´ng nghiá»‡p náº·ng nhÆ° Ã´tÃ´, hÃ ng khÃ´ng, cÃ¡c ngÃ nh nÄƒng lÆ°á»£ng, Ä‘iá»‡n lá»±c, sáº£n xuáº¥t thÃ´ng minh, tá»‘i Æ°u hÃ³a quy trÃ¬nh, cÅ©ng nhÆ° cÃ¡c lÄ©nh vá»±c tÃ i chÃ­nh kinh táº¿. Trong sá»‘ cÃ¡c á»©ng dá»¥ng AI Ä‘Æ°á»£c biáº¿t Ä‘áº¿n rá»™ng rÃ£i hiá»‡n nay cÃ³ thá»ƒ ká»ƒ Ä‘áº¿n cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n nhÆ° ChatGPT, Gemini hay cÃ¡c mÃ´ hÃ¬nh táº¡o sinh áº£nh, video nhÆ° Sora, DALL-E, Stable Diffusionâ€¦ Äáº·c Ä‘iá»ƒm cá»§a cÃ¡c mÃ´ hÃ¬nh nÃ y lÃ  cÃ¡ch huáº¥n luyá»‡n cho mÃ´ hÃ¬nh tá»± há»c, tá»± Ä‘á»™ng cáº­p nháº­t cÃ¡c trá»ng sá»‘ (parameters) vÃ  tá»‹nh tiáº¿n Ä‘áº¿n Ä‘iá»ƒm tá»‘i Æ°u báº±ng sá»­ dá»¥ng cÃ¡c **thuáº­t toÃ¡n tá»‘i Æ°u hÃ³a dá»±a trÃªn Gradient**. VÃ¬ cÃ¡c trá»ng sá»‘ lÃ  do mÃ´ hÃ¬nh tá»± há»c tá»« quÃ¡ trÃ¬nh huáº¥n luyá»‡n vá»›i dá»¯ liá»‡u, nÃªn khÃ´ng pháº£i lÃºc nÃ o chÃºng ta cÅ©ng cÃ³ thá»ƒ hiá»ƒu Ä‘Æ°á»£c táº¡i sao mÃ´ hÃ¬nh láº¡i Ä‘Æ°a ra cÃ¢u tráº£ lá»i nhÆ° váº­y, cÃ³ thá»ƒ nÃ³i, lá»±a chá»n cá»§a mÃ´ hÃ¬nh lÃ  má»™t **há»™p Ä‘en** Ä‘á»‘i vá»›i chÃºng ta. HÆ¡n ná»¯a, trong cÃ¡c bÃ i toÃ¡n thá»±c táº¿ cÃ³ Ä‘á»™ phá»©c táº¡p cao, khÃ´ng pháº£i lÃºc nÃ o chÃºng ta cÅ©ng cÃ³ thá»ƒ dÃ¹ng Gradient Descent Ä‘á»ƒ tÃ¬m ra Ä‘iá»ƒm tá»‘i Æ°u.

## 1.1. BÃ i toÃ¡n tá»‘i Æ°u trong thá»±c táº¿

Láº¥y má»™t vÃ­ dá»¥ vá» bÃ i toÃ¡n tá»‘i Æ°u trong thá»±c táº¿: tá»‘i Æ°u thiáº¿t káº¿ xe Ä‘ua lÃ  má»™t bÃ i toÃ¡n **cá»±c ká»³ phá»©c táº¡p**, khÃ´ng chá»‰ liÃªn quan Ä‘áº¿n cÃ´ng thá»©c toÃ¡n há»c mÃ  cÃ²n liÃªn quan Ä‘áº¿n váº­t lÃ½, khÃ­ Ä‘á»™ng há»c, khoa há»c váº­t liá»‡u vÃ  **hÃ ng trÄƒm biáº¿n Ä‘áº§u vÃ o phi tuyáº¿n** khÃ¡c.

### Báº£n cháº¥t cá»§a bÃ i toÃ¡n thiáº¿t káº¿ xe Ä‘ua:

Khi tá»‘i Æ°u thiáº¿t káº¿ xe Ä‘ua (vÃ­ dá»¥ F1), ta cÃ³ ráº¥t nhiá»u yáº¿u tá»‘:
- HÃ¬nh dáº¡ng khung xe (Ä‘Æ°á»ng cong, gÃ³c nghiÃªng, profile khÃ­ Ä‘á»™ng há»c).
- KÃ­ch thÆ°á»›c cÃ¡c bá»™ pháº­n (cÃ¡nh giÃ³, gáº§m, há»‘c giÃ³,...)
- Váº­t liá»‡u cáº¥u trÃºc
- Cáº¥u hÃ¬nh Ä‘á»™ng cÆ¡, há»™p sá»‘, há»‡ thá»‘ng truyá»n Ä‘á»™ng
ThÃ´ng sá»‘ Ä‘iá»u khiá»ƒn (aero setup, suspension, tire pressure, â€¦)

NhÆ° váº­y, ta cÃ³ thá»ƒ tháº¥y lÃ  bÃ i toÃ¡n tá»‘i Æ°u thiáº¿t káº¿ xe Ä‘ua lÃ  káº¿t quáº£ cá»§a má»™t mÃ´ phá»ng váº­t lÃ½ phá»©c táº¡p, chá»© khÃ´ng pháº£i lÃ  má»™t hÃ m **kháº£ vi** Ä‘Æ¡n giáº£n, vÃ  do Ä‘Ã³ phÆ°Æ¡ng Ã¡n dÃ¹ng Gradient Descent Ä‘á»ƒ tÃ¬m tá»‘i Æ°u lÃ  khÃ´ng kháº£ thi. 
Trong thá»±c táº¿, cÃ¡c hÃ£ng nhÆ° Ferrari, Mercedes-AMG Ä‘á»u dÃ¹ng **AI + GA + CFD** mÃ´ phá»ng Ä‘á»ƒ tá»‘i Æ°u thiáº¿t káº¿ khung xe, cÃ¡nh giÃ³â€¦ á» Ä‘Ã¢y, **GA** (**Genetic Algorithm**) lÃ  má»™t thuáº­t toÃ¡n thay tháº¿ cho Gradient Descent Ä‘á»ƒ tÃ¬m tá»‘i Æ°u trong cÃ¡c bÃ i toÃ¡n phá»©c táº¡p.


**Xem thÃªm:** ChÃºng ta cÃ³ thá»ƒ xem thÃªm má»™t video vá» thiáº¿t káº¿ mÃ´ phá»ng sá»­ dá»¥ng Genetic Algorithm á»Ÿ link: [AI Learns to be a Car using a Genetic Algorithm](https://www.youtube.com/watch?v=Ei2g8XoCkdg)


## 1.2. XAI trong bÃ i toÃ¡n tá»‘i Æ°u thá»±c tiá»…n

NhÆ° Ä‘Ã£ Ä‘á» cáº­p bÃªn trÃªn, má»™t trong nhá»¯ng váº¥n Ä‘á» cá»‘t lÃµi cá»§a AI lÃ  **sá»± thiáº¿u minh báº¡ch (Black-box)** vÃ  **tÃ­nh tin cáº­y**.

### TÃ­nh tin cáº­y
Khi mÃ´ hÃ¬nh AI + CFD + GA Ä‘Æ°a ra má»™t thiáº¿t káº¿ mÃ  nÃ³ cho lÃ  tá»‘i Æ°u má»›i, cÃ¡c ká»¹ sÆ° cáº§n biáº¿t ráº±ng liá»‡u thiáº¿t káº¿ Ä‘Ã³ cÃ³ thá»±c sá»± **an toÃ n** vÃ  **hiá»‡u quáº£** hay khÃ´ng, hay Ä‘Ã³ chá»‰ lÃ  má»™t káº¿t quáº£ ngáº«u nhiÃªn?

### Sá»± minh báº¡ch

Trong thiáº¿t káº¿ xe Ä‘ua, váº¥n Ä‘á» khÃ´ng chá»‰ lÃ  Ä‘áº¡t hiá»‡u suáº¥t cao nháº¥t mÃ  cÃ²n pháº£i quáº£n lÃ½ chi phÃ­ sáº£n xuáº¥t, chi phÃ­ váº­t liá»‡u vÃ  tuÃ¢n thá»§ cÃ¡c quy Ä‘á»‹nh. NÃ³i cÃ¡ch khÃ¡c, bÃ i toÃ¡n thiáº¿t káº¿ xe Ä‘ua tá»‘i Æ°u cÃ²n pháº£i thá»a mÃ£n **CÃ¢n báº±ng Chi phÃ­ vÃ  Hiá»‡u Suáº¥t** cÅ©ng nhÆ° **cÃ¡c quy Ä‘á»‹nh**.

Bá»Ÿi váº­y, viá»‡c dÃ¹ng XAI Ä‘á»ƒ giáº£i thÃ­ch Ä‘Æ°á»£c yáº¿u tá»‘ nÃ o Ä‘Ã³ng gÃ³p vÃ o thiáº¿t káº¿ tá»‘i Æ°u bao nhiÃªu pháº§n trÄƒm Ä‘Ã³ng má»™t vai trÃ² cá»±c ká»³ quan trá»ng. Vi dá»¥: Náº¿u mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n ráº±ng má»™t váº­t liá»‡u siÃªu Ä‘áº¯t tiá»n chá»‰ cáº£i thiá»‡n hiá»‡u suáº¥t 0.5% nhÆ°ng má»™t thay Ä‘á»•i nhá» vá» hÃ¬nh há»c gáº§m xe láº¡i cáº£i thiá»‡n 5%, XAI giÃºp nhÃ³m thiáº¿t káº¿ Ä‘Æ°a ra lá»±a chá»n tá»‘i Æ°u pháº§n nÃ o vá»›i chi phÃ­ giá»›i háº¡n (chá»n giáº£i phÃ¡p hÃ¬nh há»c ráº» hÆ¡n, hiá»‡u quáº£ hÆ¡n).

Bá»Ÿi váº­y ngoÃ i thuáº­t toÃ¡n di truyá»n GA, thÃ¬ cÃ²n cáº§n káº¿t há»£p thÃªm thuáº­t toÃ¡n SHAP Ä‘á»ƒ hiá»ƒu rÃµ yáº¿u tá»‘ nÃ o Ä‘Ã³ng gÃ³p bao nhiÃªu pháº§n trÄƒm vÃ o má»™t bÃ i toÃ¡n tá»‘i Æ°u. Má»™t há»‡ thá»‘ng gá»“m **AI + GA +CFD + SHAP** cÃ³ thá»ƒ giáº£m thá»i gian thiáº¿t káº¿ tá»« hÃ ng thÃ¡ng xuá»‘ng cÃ²n vÃ i ngÃ y vÃ  cÅ©ng giÃºp tiáº¿t kiá»‡m hÃ ng triá»‡u Ä‘Ã´ chi phÃ­ mÃ´ phá»ng. Trong cÃ¡c pháº§n tiáº¿p theo cá»§a blog, chÃºng ta sáº½ cÃ¹ng tÃ¬m hiá»ƒu ká»¹ hÆ¡n vá» GA, SHAP cÅ©ng nhÆ° cÃ¡c biáº¿n thá»ƒ cá»§a nÃ³.

# 2. Giá»›i thiá»‡u Genetic Algorithm (GA)

## 2.1. Genetic Algorithm lÃ  gÃ¬?

**Genetic Algorithm (GA)** lÃ  thuáº­t toÃ¡n tá»‘i Æ°u hÃ³a dá»±a trÃªn nguyÃªn lÃ½ **chá»n lá»c tá»± nhiÃªn** vÃ  **di truyá»n há»c** cá»§a Darwin. GA mÃ´ phá»ng quÃ¡ trÃ¬nh tiáº¿n hÃ³a Ä‘á»ƒ tÃ¬m nghiá»‡m tá»‘i Æ°u cho cÃ¡c bÃ i toÃ¡n phá»©c táº¡p.

### Ã tÆ°á»Ÿng:

- CÃ¡ thá»ƒ **máº¡nh** (nghiá»‡m tá»‘t) cÃ³ kháº£ nÄƒng **sá»‘ng sÃ³t** vÃ  **sinh sáº£n** cao hÆ¡n
- ThÃ´ng qua **lai ghÃ©p** vÃ  **Ä‘á»™t biáº¿n**, tháº¿ há»‡ sau tá»‘t hÆ¡n tháº¿ há»‡ trÆ°á»›c
- Sau nhiá»u tháº¿ há»‡, quáº§n thá»ƒ tiáº¿n hÃ³a vá» nghiá»‡m tá»‘i Æ°u

### Táº¡i sao cáº§n Genetic Algorithm?

Trong cÃ¡c bÃ i toÃ¡n tá»‘i Æ°u cá»• Ä‘iá»ƒn, khi biáº¿t rÃµ hÃ m má»¥c tiÃªu vÃ  cÃ³ thá»ƒ tÃ­nh Ä‘áº¡o hÃ m, **Gradient Descent (GD)** lÃ  lá»±a chá»n tá»± nhiÃªn. Tuy nhiÃªn, thá»±c táº¿ thÆ°á»ng phá»©c táº¡p hÆ¡n:

- HÃ m má»¥c tiÃªu **khÃ´ng kháº£ vi** (discrete, cÃ³ bÆ°á»›c nháº£y)
- Dá»… bá»‹ **káº¹t á»Ÿ cá»±c trá»‹ cá»¥c bá»™** (local optima)
- KhÃ´ng gian tÃ¬m kiáº¿m **phá»©c táº¡p**, nhiá»u chiá»u

**Genetic Algorithm (GA)** ra Ä‘á»i Ä‘á»ƒ giáº£i quyáº¿t nhá»¯ng háº¡n cháº¿ nÃ y.

### So sÃ¡nh GD vs GA:

| TiÃªu chÃ­                        | Gradient Descent         | Genetic Algorithm             |
| ------------------------------- | ------------------------ | ----------------------------- |
| **CÃ¡ch tiáº¿p cáº­n**               | Äi theo 1 quá»¹ Ä‘áº¡o Ä‘Æ¡n láº» | Duy trÃ¬ cáº£ quáº§n thá»ƒ nghiá»‡m    |
| **YÃªu cáº§u**                     | Cáº§n Ä‘áº¡o hÃ m              | KhÃ´ng cáº§n Ä‘áº¡o hÃ m             |
| **Kháº£ nÄƒng thoÃ¡t local optima** | Tháº¥p                     | Cao (nhá» Ä‘á»™t biáº¿n vÃ  Ä‘a dáº¡ng) |
| **Tá»‘c Ä‘á»™ há»™i tá»¥**               | Nhanh                    | Cháº­m hÆ¡n                      |
| **PhÃ¹ há»£p**                     | HÃ m liÃªn tá»¥c, kháº£ vi     | HÃ m rá»i ráº¡c, phá»©c táº¡p         |

## 2.2. Quy trÃ¬nh hoáº¡t Ä‘á»™ng cá»§a GA

Quy trÃ¬nh cá»§a GA cÃ³ thá»ƒ Ä‘Æ°á»£c mÃ´ táº£ qua cÃ¡c bÆ°á»›c sau:

### **BÆ°á»›c 1: Khá»Ÿi táº¡o quáº§n thá»ƒ (Initialization)**

Táº¡o ngáº«u nhiÃªn **má»™t táº­p há»£p cÃ¡c cÃ¡ thá»ƒ (individuals)** â€” má»—i cÃ¡ thá»ƒ biá»ƒu diá»…n má»™t **lá»i giáº£i tiá»m nÄƒng** cho bÃ i toÃ¡n.  
VÃ­ dá»¥: náº¿u bÃ i toÃ¡n cáº§n tÃ¬m bá»™ tham sá»‘ tá»‘i Æ°u, thÃ¬ má»—i cÃ¡ thá»ƒ cÃ³ thá»ƒ lÃ  má»™t vector giÃ¡ trá»‹ `[xâ‚, xâ‚‚, ..., xn]`.

### **BÆ°á»›c 2: ÄÃ¡nh giÃ¡ fitness quáº§n thá»ƒ (Fitness Evaluation)**

Sá»­ dá»¥ng **hÃ m má»¥c tiÃªu (fitness function)** Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ â€œÄ‘á»™ tá»‘tâ€ cá»§a tá»«ng cÃ¡ thá»ƒ.

- CÃ¡ thá»ƒ cÃ³ Ä‘iá»ƒm fitness cao thá»ƒ hiá»‡n lá»i giáº£i gáº§n tá»‘i Æ°u hÆ¡n.
- HÃ m fitness Ä‘Æ°á»£c thiáº¿t káº¿ tÃ¹y thuá»™c vÃ o bÃ i toÃ¡n cá»¥ thá»ƒ (vÃ­ dá»¥: lá»£i nhuáº­n, sai sá»‘, thá»i gian, Ä‘á»™ chÃ­nh xÃ¡c...).

### **BÆ°á»›c 3: Giá»¯ láº¡i cÃ¡ thá»ƒ Æ°u tÃº (Elitism)**

Má»™t sá»‘ cÃ¡ thá»ƒ tá»‘t nháº¥t (elite) Ä‘Æ°á»£c **giá»¯ nguyÃªn** sang tháº¿ há»‡ káº¿ tiáº¿p Ä‘á»ƒ Ä‘áº£m báº£o cháº¥t lÆ°á»£ng khÃ´ng giáº£m sÃºt.

### **BÆ°á»›c 4: Chá»n lá»c bá»‘ máº¹ (Selection)**

Chá»n cÃ¡c cÃ¡ thá»ƒ tá»« quáº§n thá»ƒ hiá»‡n táº¡i Ä‘á»ƒ lÃ m **bá»‘ máº¹** dá»±a trÃªn Ä‘iá»ƒm fitness.  
CÃ¡c phÆ°Æ¡ng phÃ¡p chá»n lá»c phá»• biáº¿n:

- **Roulette Wheel Selection** (xÃ¡c suáº¥t tá»‰ lá»‡ vá»›i fitness)
- **Tournament Selection** (chá»n cÃ¡ thá»ƒ tá»‘t nháº¥t trong nhÃ³m ngáº«u nhiÃªn)

### **BÆ°á»›c 5: Lai ghÃ©p (Crossover)**

Hai cÃ¡ thá»ƒ bá»‘ máº¹ **káº¿t há»£p gen** Ä‘á»ƒ táº¡o ra **cÃ¡c cÃ¡ thá»ƒ con**.

- Má»¥c tiÃªu: khai thÃ¡c thÃ´ng tin di truyá»n tá»‘t tá»« bá»‘ máº¹.
- VÃ­ dá»¥:
  - Cha: [1, 0, 1, 1]
  - Máº¹: [0, 1, 0, 0]
  - Con: [1, 0, 0, 0]

### **BÆ°á»›c 6: Äá»™t biáº¿n (Mutation)**

Má»™t sá»‘ gen trong cÃ¡ thá»ƒ con Ä‘Æ°á»£c **thay Ä‘á»•i ngáº«u nhiÃªn** Ä‘á»ƒ tÄƒng **Ä‘a dáº¡ng di truyá»n** vÃ  giÃºp trÃ¡nh rÆ¡i vÃ o **cá»±c trá»‹ cá»¥c bá»™**.  
VÃ­ dá»¥: thay Ä‘á»•i 1 bit trong chuá»—i gen `[1, 0, 0, 0] â†’ [1, 1, 0, 0]`.

### **BÆ°á»›c 7: Cáº­p nháº­t quáº§n thá»ƒ má»›i (New Generation)**

Sau khi lai ghÃ©p vÃ  Ä‘á»™t biáº¿n, ta cÃ³ má»™t **tháº¿ há»‡ má»›i** gá»“m:

- CÃ¡ thá»ƒ Æ°u tÃº Ä‘Æ°á»£c giá»¯ láº¡i
- CÃ¡c cÃ¡ thá»ƒ con má»›i Ä‘Æ°á»£c sinh ra  
  â†’ ÄÃ¢y chÃ­nh lÃ  quáº§n thá»ƒ sáº½ Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ trong vÃ²ng láº·p tiáº¿p theo.

### **BÆ°á»›c 8: Kiá»ƒm tra Ä‘iá»u kiá»‡n dá»«ng (Termination Condition)**

Thuáº­t toÃ¡n sáº½ dá»«ng khi:

- Äáº¡t sá»‘ tháº¿ há»‡ tá»‘i Ä‘a, **hoáº·c**
- Äá»™ cáº£i thiá»‡n fitness khÃ´ng Ä‘Ã¡ng ká»ƒ, **hoáº·c**
- ÄÃ£ tÃ¬m tháº¥y lá»i giáº£i Ä‘á»§ tá»‘t (thoáº£ Ä‘iá»u kiá»‡n má»¥c tiÃªu).

Náº¿u **chÆ°a Ä‘áº¡t**, quay láº¡i **BÆ°á»›c 2** Ä‘á»ƒ láº·p láº¡i tiáº¿n trÃ¬nh.

### **BÆ°á»›c 9: Káº¿t thÃºc**

Tráº£ vá» **cÃ¡ thá»ƒ cÃ³ fitness cao nháº¥t** â€” Ä‘Ã³ chÃ­nh lÃ  **lá»i giáº£i tá»‘i Æ°u (hoáº·c gáº§n tá»‘i Æ°u)** mÃ  GA tÃ¬m Ä‘Æ°á»£c.

## 2.3. Æ¯u vÃ  nhÆ°á»£c Ä‘iá»ƒm

### Æ¯u Ä‘iá»ƒm:

- KhÃ´ng cáº§n gradient (phÃ¹ há»£p vá»›i hÃ m rá»i ráº¡c, khÃ´ng kháº£ vi)  
- TÃ¬m kiáº¿m global optimum tá»‘t hÆ¡n cÃ¡c phÆ°Æ¡ng phÃ¡p greedy  
- Linh hoáº¡t, Ã¡p dá»¥ng Ä‘Æ°á»£c cho nhiá»u bÃ i toÃ¡n  
- Dá»… song song hÃ³a (Ä‘Ã¡nh giÃ¡ fitness Ä‘á»™c láº­p)

### NhÆ°á»£c Ä‘iá»ƒm:

- Tá»‘n thá»i gian tÃ­nh toÃ¡n (nhiá»u tháº¿ há»‡)  
- Cáº§n chá»n hyperparameter (kÃ­ch thÆ°á»›c quáº§n thá»ƒ, mutation rate, ...)  
- KhÃ´ng Ä‘áº£m báº£o tÃ¬m Ä‘Æ°á»£c nghiá»‡m tá»‘i Æ°u tuyá»‡t Ä‘á»‘i  
- KhÃ³ Ä‘iá»u chá»‰nh cho bÃ i toÃ¡n cá»¥ thá»ƒ

## 2.4. á»¨ng dá»¥ng cá»§a GA trong Machine Learning

| á»¨ng dá»¥ng                       | MÃ´ táº£                                           |
| ------------------------------ | ----------------------------------------------- |
| **Feature Selection**          | Chá»n táº­p con Ä‘áº·c trÆ°ng tá»‘i Æ°u                   |
| **Hyperparameter Tuning**      | Tá»‘i Æ°u learning rate, batch size, sá»‘ layers,... |
| **Neural Architecture Search** | TÃ¬m kiáº¿m cáº¥u trÃºc máº¡ng neural tá»‘i Æ°u            |
| **Ensemble Learning**          | Chá»n vÃ  káº¿t há»£p cÃ¡c model cÆ¡ sá»Ÿ                 |
| **Data Augmentation**          | TÃ¬m chiáº¿n lÆ°á»£c augmentation tá»‘t nháº¥t            |

## 2.5. So sÃ¡nh GA vá»›i cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c

| PhÆ°Æ¡ng phÃ¡p               | Æ¯u Ä‘iá»ƒm               | NhÆ°á»£c Ä‘iá»ƒm                 | Khi nÃ o dÃ¹ng                  |
| ------------------------- | --------------------- | -------------------------- | ----------------------------- |
| **Grid Search**           | ÄÆ¡n giáº£n, toÃ n diá»‡n   | Ráº¥t cháº­m vá»›i nhiá»u tham sá»‘ | KhÃ´ng gian nhá» (< 5 tham sá»‘)  |
| **Random Search**         | Nhanh hÆ¡n Grid        | KhÃ´ng táº­n dá»¥ng thÃ´ng tin   | KhÃ´ng gian vá»«a (5-10 tham sá»‘) |
| **Bayesian Optimization** | Hiá»‡u quáº£, Ã­t Ä‘Ã¡nh giÃ¡ | Phá»©c táº¡p, khÃ³ cÃ i Ä‘áº·t      | HÃ m Ä‘áº¯t Ä‘á»ƒ tÃ­nh toÃ¡n          |
| **Genetic Algorithm**     | Linh hoáº¡t, tÃ¬m global | Nhiá»u hyperparameter       | KhÃ´ng gian lá»›n, phá»©c táº¡p      |

## 2.6. Tham sá»‘ quan trá»ng cáº§n Ä‘iá»u chá»‰nh

| Tham sá»‘                | GiÃ¡ trá»‹ thÆ°á»ng dÃ¹ng | áº¢nh hÆ°á»Ÿng                          |
| ---------------------- | ------------------- | ---------------------------------- |
| **Population size**    | 50-200              | Lá»›n â†’ Ä‘a dáº¡ng nhÆ°ng cháº­m           |
| **Crossover rate**     | 0.6-0.9             | Cao â†’ khai thÃ¡c nghiá»‡m tá»‘t         |
| **Mutation rate**      | 0.01-0.1            | Cao â†’ khÃ¡m phÃ¡, trÃ¡nh local optima |
| **Generations**        | 50-500              | Nhiá»u â†’ chÃ­nh xÃ¡c nhÆ°ng cháº­m       |
| **Selection pressure** | Tournament size 3-5 | Cao â†’ há»™i tá»¥ nhanh                 |

## 2.7. TÃ³m táº¯t

- **GA** mÃ´ phá»ng tiáº¿n hÃ³a tá»± nhiÃªn Ä‘á»ƒ tá»‘i Æ°u hÃ³a
- **5 bÆ°á»›c chÃ­nh**: Khá»Ÿi táº¡o â†’ ÄÃ¡nh giÃ¡ â†’ Chá»n lá»c â†’ Lai ghÃ©p â†’ Äá»™t biáº¿n
- **PhÃ¹ há»£p** vá»›i bÃ i toÃ¡n khÃ´ng gian lá»›n, phá»©c táº¡p, khÃ´ng kháº£ vi
- **á»¨ng dá»¥ng** nhiá»u trong ML: feature selection, hyperparameter tuning,...
- **LÆ°u Ã½**: Cáº§n Ä‘iá»u chá»‰nh tham sá»‘ phÃ¹ há»£p vá»›i tá»«ng bÃ i toÃ¡n cá»¥ thá»ƒ

# 3. Giá»›i thiá»‡u SHAP (dá»±a trÃªn Shapley Values)

## 3.1. Khi AI trá»Ÿ nÃªn â€œbÃ­ áº©nâ€ â€“ váº¥n Ä‘á» vá» kháº£ nÄƒng giáº£i thÃ­ch  

HÃ£y tÆ°á»Ÿng tÆ°á»£ng má»™t buá»•i sÃ¡ng, báº¡n má»Ÿ á»©ng dá»¥ng ngÃ¢n hÃ ng vÃ  tháº¥y dÃ²ng thÃ´ng bÃ¡o:  

> â€œYÃªu cáº§u vay tÃ­n dá»¥ng cá»§a báº¡n bá»‹ tá»« chá»‘i.â€  

KhÃ´ng cÃ³ lá»i giáº£i thÃ­ch. KhÃ´ng biáº¿t vÃ¬ sao.  
Dá»¯ liá»‡u báº¡n cung cáº¥p ráº¥t á»•n: thu nháº­p tá»‘t, cÃ´ng viá»‡c á»•n Ä‘á»‹nh, khÃ´ng ná»£ xáº¥u.  
NhÆ°ng há»‡ thá»‘ng AI chá»‰ tráº£ vá» má»™t cÃ¢u tráº£ lá»i dá»©t khoÃ¡t: **â€œKhÃ´ng.â€**

ÄÃ¢y chÃ­nh lÃ  má»™t vÃ­ dá»¥ Ä‘iá»ƒn hÃ¬nh cá»§a **váº¥n Ä‘á» thiáº¿u kháº£ nÄƒng giáº£i thÃ­ch (Explainability)** trong cÃ¡c há»‡ thá»‘ng **trÃ­ tuá»‡ nhÃ¢n táº¡o hiá»‡n Ä‘áº¡i**.  
Nhá»¯ng mÃ´ hÃ¬nh phá»©c táº¡p nhÆ° **XGBoost, Random Forest, hay Deep Neural Network** cÃ³ thá»ƒ Ä‘áº¡t Ä‘á»™ chÃ­nh xÃ¡c cao,  
nhÆ°ng láº¡i hoáº¡t Ä‘á»™ng nhÆ° **â€œhá»™p Ä‘enâ€ (black box)** â€” ta chá»‰ biáº¿t Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra, mÃ  khÃ´ng thá»ƒ nhÃ¬n tháº¥y bÃªn trong.  

Thá»±c ra, Ä‘iá»u nÃ y **khÃ´ng xa láº¡ vá»›i chÃ­nh con ngÆ°á»i**.  
**Bá»™ nÃ£o** cá»§a chÃºng ta cÅ©ng lÃ  má»™t **â€œhá»™p Ä‘en tá»‘i thÆ°á»£ng.â€**  
CÃ¡c nhÃ  khoa há»c tháº§n kinh cÃ³ thá»ƒ nÃ³i:  
> â€œVá» nÃ£o trÆ°á»›c trÃ¡n hoáº¡t Ä‘á»™ng khi ta láº­p káº¿ hoáº¡ch,â€  
> hay â€œdopamine liÃªn quan Ä‘áº¿n cáº£m giÃ¡c pháº§n thÆ°á»Ÿng.â€  

NhÆ°ng há» váº«n **chÆ°a thá»ƒ giáº£i thÃ­ch chÃ­nh xÃ¡c** cÃ¡ch hÃ ng tá»· neuron trong nÃ£o káº¿t ná»‘i vÃ  tÆ°Æ¡ng tÃ¡c Ä‘á»ƒ Ä‘Æ°a ra **má»™t quyáº¿t Ä‘á»‹nh cá»¥ thá»ƒ**.  

TÆ°Æ¡ng tá»±, **AI cÅ©ng váº­y.**  
ChÃºng ta cÃ³ thá»ƒ biáº¿t ráº±ng â€œfeature A tÄƒng â†’ xÃ¡c suáº¥t B cao hÆ¡n,â€  
nhÆ°ng khÃ´ng dá»… Ä‘á»ƒ hiá»ƒu **vÃ¬ sao mÃ´ hÃ¬nh láº¡i chá»n hÆ°á»›ng Ä‘Ã³**, hay **má»—i Ä‘áº·c trÆ°ng áº£nh hÆ°á»Ÿng Ä‘áº¿n káº¿t quáº£ bao nhiÃªu**.  

VÃ¬ tháº¿, cÅ©ng nhÆ° **khoa há»c tháº§n kinh Ä‘ang cá»‘ â€œgiáº£i mÃ£ nÃ£o bá»™â€**,  
**Explainable AI (XAI)** chÃ­nh lÃ  ná»— lá»±c â€œgiáº£i mÃ£ tÆ° duy cá»§a mÃ¡y tÃ­nh.â€  
NÃ³ giÃºp con ngÆ°á»i hiá»ƒu Ä‘Æ°á»£c cÃ¡ch mÃ  cÃ¡c mÃ´ hÃ¬nh há»c mÃ¡y **ra quyáº¿t Ä‘á»‹nh, cÃ¢n nháº¯c, vÃ  pháº£n á»©ng** trÆ°á»›c dá»¯ liá»‡u.  

VÃ  trong hÃ nh trÃ¬nh nÃ y, **SHAP (SHapley Additive exPlanations)**  
lÃ  má»™t trong nhá»¯ng phÆ°Æ¡ng phÃ¡p quan trá»ng nháº¥t,  
vÃ¬ nÃ³ khÃ´ng chá»‰ nÃ³i â€œmÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng tháº¿ nÃ oâ€, mÃ  cÃ²n **Ä‘á»‹nh lÆ°á»£ng Ä‘Æ°á»£c tá»«ng pháº§n Ä‘Ã³ng gÃ³p** cá»§a má»—i Ä‘áº·c trÆ°ng â€”  
tÆ°Æ¡ng tá»± nhÆ° viá»‡c tÃ¬m hiá»ƒu tá»«ng neuron trong nÃ£o áº£nh hÆ°á»Ÿng tháº¿ nÃ o Ä‘áº¿n má»™t hÃ nh Ä‘á»™ng cá»¥ thá»ƒ.

---

## 3.2. Tá»« lÃ½ thuyáº¿t trÃ² chÆ¡i Ä‘áº¿n mÃ¡y há»c â€“ cÃ¢u chuyá»‡n cá»§a SHAP  

SHAP dá»±a trÃªn má»™t Ã½ tÆ°á»Ÿng xuáº¥t phÃ¡t tá»« **lÃ½ thuyáº¿t trÃ² chÆ¡i há»£p tÃ¡c (Cooperative Game Theory)** do nhÃ  toÃ¡n há»c **Lloyd Shapley** Ä‘á» xuáº¥t nÄƒm 1953.  

Giáº£ sá»­ báº¡n cÃ³ má»™t Ä‘á»™i bÃ³ng gá»“m ba cáº§u thá»§: An, BÃ¬nh, vÃ  Chi.  
Cáº£ Ä‘á»™i tháº¯ng vÃ  nháº­n Ä‘Æ°á»£c 100 Ä‘iá»ƒm thÆ°á»Ÿng. NhÆ°ng ai Ä‘Ã³ng gÃ³p bao nhiÃªu?  
Náº¿u chá»‰ chia Ä‘á»u, cÃ³ thá»ƒ khÃ´ng cÃ´ng báº±ng â€“ vÃ¬ ngÆ°á»i ghi bÃ n hay ngÆ°á»i giá»¯ khung thÃ nh Ä‘á»u áº£nh hÆ°á»Ÿng khÃ¡c nhau Ä‘áº¿n káº¿t quáº£ cuá»‘i cÃ¹ng.  

Äá»ƒ chia thÆ°á»Ÿng há»£p lÃ½, Shapley Ä‘Æ°a ra cÃ¡ch tiáº¿p cáº­n:  
- Thá»­ loáº¡i tá»«ng ngÆ°á»i khá»i Ä‘á»™i vÃ  xem **Ä‘á»™i cÃ²n láº¡i chÆ¡i tá»‘t Ä‘áº¿n Ä‘Ã¢u**.  
- TÃ­nh **má»©c chÃªnh lá»‡ch trong thÃ nh tÃ­ch** khi ngÆ°á»i Ä‘Ã³ tham gia hoáº·c khÃ´ng tham gia.  
- Láº¥y trung bÃ¬nh trÃªn **táº¥t cáº£ cÃ¡c cÃ¡ch káº¿t há»£p cÃ³ thá»ƒ**.  

Káº¿t quáº£ lÃ , ta biáº¿t Ä‘Æ°á»£c **Ä‘Ã³ng gÃ³p cÃ´ng báº±ng cá»§a tá»«ng ngÆ°á»i chÆ¡i**.  

Trong **trÃ­ tuá»‡ nhÃ¢n táº¡o**, â€œtrÃ² chÆ¡iâ€ chÃ­nh lÃ  mÃ´ hÃ¬nh há»c mÃ¡y,  
vÃ  â€œngÆ°á»i chÆ¡iâ€ chÃ­nh lÃ  **cÃ¡c Ä‘áº·c trÆ°ng (features)** trong táº­p dá»¯ liá»‡u.  
SHAP Ä‘Ã³ng vai trÃ² lÃ  â€œtrá»ng tÃ iâ€ tÃ­nh **Ä‘á»™ Ä‘Ã³ng gÃ³p cÃ´ng báº±ng cá»§a tá»«ng feature** vÃ o káº¿t quáº£ dá»± Ä‘oÃ¡n.

---

## 3.3. NguyÃªn lÃ½ hoáº¡t Ä‘á»™ng cá»§a SHAP â€“ chia pháº§n cÃ´ng báº±ng cho tá»«ng Ä‘áº·c trÆ°ng  

### (1) Ã tÆ°á»Ÿng cÆ¡ báº£n  

HÃ£y xem mÃ´ hÃ¬nh há»c mÃ¡y nhÆ° má»™t trÃ² chÆ¡i há»£p tÃ¡c giá»¯a cÃ¡c Ä‘áº·c trÆ°ng dá»¯ liá»‡u.  
Má»—i Ä‘áº·c trÆ°ng Ä‘Ã³ng gÃ³p má»™t pháº§n vÃ o â€œpháº§n thÆ°á»Ÿng cuá»‘i cÃ¹ngâ€ â€” tá»©c lÃ  **káº¿t quáº£ dá»± Ä‘oÃ¡n**.

Äá»ƒ Ä‘o Ä‘Ã³ng gÃ³p cá»§a má»™t Ä‘áº·c trÆ°ng i, SHAP xem xÃ©t:
- Náº¿u **Ä‘áº·c trÆ°ng Ä‘Ã³ cÃ³ máº·t**, dá»± Ä‘oÃ¡n thay Ä‘á»•i nhÆ° tháº¿ nÃ o?  
- Náº¿u **Ä‘áº·c trÆ°ng Ä‘Ã³ bá»‹ loáº¡i bá»**, mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng ra sao?  

Báº±ng cÃ¡ch tÃ­nh **má»©c thay Ä‘á»•i trung bÃ¬nh** cá»§a káº¿t quáº£ qua **má»i tá»• há»£p Ä‘áº·c trÆ°ng cÃ³ thá»ƒ cÃ³**,  
SHAP xÃ¡c Ä‘á»‹nh **Ä‘Ã³ng gÃ³p thá»±c sá»±** cá»§a tá»«ng Ä‘áº·c trÆ°ng Ä‘á»‘i vá»›i quyáº¿t Ä‘á»‹nh cuá»‘i cÃ¹ng.

---

### (2) CÃ´ng thá»©c tá»•ng quÃ¡t  

$$
Ï•_i = \sum_{S \subseteq N \setminus \{i\}} \frac{|S|!(M - |S| - 1)!}{M!} [ f(S âˆª \{i\}) - f(S) ]
$$

Trong Ä‘Ã³:  
- **N**: táº­p há»£p toÃ n bá»™ Ä‘áº·c trÆ°ng  
- **S**: má»™t táº­p con cá»§a cÃ¡c Ä‘áº·c trÆ°ng khÃ´ng chá»©a i  
- **f(S)**: giÃ¡ trá»‹ dá»± Ä‘oÃ¡n khi mÃ´ hÃ¬nh chá»‰ xem xÃ©t cÃ¡c Ä‘áº·c trÆ°ng trong S  
- **Ï•áµ¢**: giÃ¡ trá»‹ SHAP cá»§a Ä‘áº·c trÆ°ng i  

CÃ´ng thá»©c nÃ y thá»ƒ hiá»‡n rÃµ triáº¿t lÃ½ â€œcÃ´ng báº±ngâ€ cá»§a Shapley:  
> Má»—i Ä‘áº·c trÆ°ng Ä‘Æ°á»£c chia pháº§n tÆ°Æ¡ng á»©ng vá»›i **má»©c áº£nh hÆ°á»Ÿng trung bÃ¬nh** cá»§a nÃ³,  
> Ä‘Æ°á»£c tÃ­nh trÃªn **táº¥t cáº£ cÃ¡c cÃ¡ch tham gia vÃ o mÃ´ hÃ¬nh**.

---

### (3) VÃ­ dá»¥ trá»±c quan  

Giáº£ sá»­ ta cÃ³ má»™t mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n **giÃ¡ nhÃ ** dá»±a trÃªn ba Ä‘áº·c trÆ°ng:
- Diá»‡n tÃ­ch (xâ‚)  
- Sá»‘ phÃ²ng ngá»§ (xâ‚‚)  
- Vá»‹ trÃ­ (xâ‚ƒ)

Khi chá»‰ dÃ¹ng (xâ‚, xâ‚‚), mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n **2.5 tá»·**.  
Khi thÃªm â€œvá»‹ trÃ­â€ (xâ‚ƒ), dá»± Ä‘oÃ¡n tÄƒng lÃªn **3.0 tá»·**.  
â†’ ÄÃ³ng gÃ³p cá»§a xâ‚ƒ lÃ  **+0.5 tá»·**.  

Náº¿u ta lÃ m tÆ°Æ¡ng tá»± cho má»i tá»• há»£p khÃ¡c (chá»‰ xâ‚, chá»‰ xâ‚ƒ, xâ‚+xâ‚ƒ, v.v.),  
rá»“i láº¥y trung bÃ¬nh cÃ³ trá»ng sá»‘ trÃªn táº¥t cáº£, ta thu Ä‘Æ°á»£c giÃ¡ trá»‹ SHAP cuá»‘i cÃ¹ng cho â€œvá»‹ trÃ­â€.  

Khi thá»±c hiá»‡n Ä‘iá»u nÃ y cho táº¥t cáº£ cÃ¡c Ä‘áº·c trÆ°ng, ta thu Ä‘Æ°á»£c mÃ´ hÃ¬nh giáº£i thÃ­ch tuyáº¿n tÃ­nh:

$$
f(x) = Ï•â‚€ + Ï•â‚ + Ï•â‚‚ + Ï•â‚ƒ
$$

Trong Ä‘Ã³:  
- Ï•â‚€: giÃ¡ trá»‹ dá»± Ä‘oÃ¡n trung bÃ¬nh (baseline)  
- Ï•â‚, Ï•â‚‚, Ï•â‚ƒ: Ä‘Ã³ng gÃ³p cá»¥ thá»ƒ cá»§a tá»«ng Ä‘áº·c trÆ°ng  

---

### (4) Minh há»a thá»±c táº¿ â€“ vÃ­ dá»¥ dá»± Ä‘oÃ¡n phÃª duyá»‡t vay  

Giáº£ sá»­ má»™t ngÃ¢n hÃ ng dÃ¹ng mÃ´ hÃ¬nh AI Ä‘á»ƒ dá»± Ä‘oÃ¡n kháº£ nÄƒng khÃ¡ch hÃ ng Ä‘Æ°á»£c duyá»‡t vay.

| Äáº·c trÆ°ng | GiÃ¡ trá»‹ SHAP (Ä‘Ã³ng gÃ³p) | áº¢nh hÆ°á»Ÿng |
|------------|--------------------------|------------|
| GiÃ¡ trá»‹ trung bÃ¬nh (baseline) | 0.30 | Máº·c Ä‘á»‹nh 30% cÆ¡ há»™i |
| Tuá»•i | +0.10 | Tuá»•i phÃ¹ há»£p tÄƒng kháº£ nÄƒng duyá»‡t |
| Thu nháº­p | +0.20 | Thu nháº­p cao giÃºp tÄƒng Ä‘iá»ƒm tÃ­n dá»¥ng |
| Ná»£ xáº¥u | âˆ’0.05 | Há»“ sÆ¡ ná»£ xáº¥u kÃ©o Ä‘iá»ƒm xuá»‘ng |

Cá»™ng táº¥t cáº£ láº¡i:

$$
f(x) = 0.30 + 0.10 + 0.20 âˆ’ 0.05 = 0.55
$$

â†’ MÃ´ hÃ¬nh dá»± Ä‘oÃ¡n ráº±ng khÃ¡ch hÃ ng cÃ³ **55% kháº£ nÄƒng Ä‘Æ°á»£c duyá»‡t vay**.  
KhÃ´ng chá»‰ cÃ³ káº¿t quáº£, mÃ  giá» ta **hiá»ƒu rÃµ â€œvÃ¬ saoâ€**:  
thu nháº­p vÃ  tuá»•i Ä‘Ã³ng gÃ³p tÃ­ch cá»±c, trong khi lá»‹ch sá»­ ná»£ xáº¥u lÃ m giáº£m Ä‘iá»ƒm.

---

### (5) TÃ­nh cháº¥t Ä‘áº·c biá»‡t cá»§a SHAP  

SHAP ná»•i báº­t nhá» ba tÃ­nh cháº¥t lÃ½ thuyáº¿t Ä‘Æ°á»£c chá»©ng minh:  

| NguyÃªn táº¯c | Ã nghÄ©a |
|-------------|----------|
| **Local Accuracy** | Tá»•ng cÃ¡c giÃ¡ trá»‹ SHAP Ä‘Ãºng báº±ng sai lá»‡ch giá»¯a dá»± Ä‘oÃ¡n vÃ  giÃ¡ trá»‹ trung bÃ¬nh. |
| **Missingness** | Náº¿u má»™t Ä‘áº·c trÆ°ng khÃ´ng xuáº¥t hiá»‡n, Ä‘Ã³ng gÃ³p cá»§a nÃ³ báº±ng 0. |
| **Consistency** | Náº¿u má»™t Ä‘áº·c trÆ°ng trá»Ÿ nÃªn quan trá»ng hÆ¡n trong mÃ´ hÃ¬nh má»›i, giÃ¡ trá»‹ SHAP cá»§a nÃ³ khÃ´ng Ä‘Æ°á»£c giáº£m Ä‘i. |

Nhá» tuÃ¢n theo ba nguyÃªn táº¯c nÃ y, SHAP Ä‘Æ°á»£c xem lÃ  **phÆ°Æ¡ng phÃ¡p duy nháº¥t trong nhÃ³m XAI Ä‘áº£m báº£o tÃ­nh cÃ´ng báº±ng vÃ  minh báº¡ch.**

---

### (6) MÃ£ minh há»a trá»±c quan trÃªn Google Colab  

```python
# CÃ i Ä‘áº·t thÆ° viá»‡n
!pip install shap xgboost

import shap
import xgboost
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_boston

# 1. Táº£i dá»¯ liá»‡u vÃ  huáº¥n luyá»‡n mÃ´ hÃ¬nh
X, y = load_boston(return_X_y=True)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = xgboost.XGBRegressor().fit(X_train, y_train)

# 2. Táº¡o SHAP explainer vÃ  tÃ­nh giÃ¡ trá»‹
explainer = shap.Explainer(model)
shap_values = explainer(X_test)

# 3. Hiá»ƒn thá»‹ biá»ƒu Ä‘á»“ tá»•ng há»£p
shap.summary_plot(shap_values, X_test, show=True)
```

Káº¿t quáº£ lÃ  **SHAP Summary Plot** â€“ biá»ƒu Ä‘á»“ thá»ƒ hiá»‡n má»©c áº£nh hÆ°á»Ÿng cá»§a tá»«ng Ä‘áº·c trÆ°ng Ä‘áº¿n káº¿t quáº£ dá»± Ä‘oÃ¡n:

- **Trá»¥c ngang:** Ä‘á»™ áº£nh hÆ°á»Ÿng (impact)  
- **MÃ u Ä‘á»:** giÃ¡ trá»‹ feature cao  
- **MÃ u xanh:** giÃ¡ trá»‹ feature tháº¥p  
- **CÃ ng xa trá»¥c 0 â†’ áº£nh hÆ°á»Ÿng cÃ ng máº¡nh Ä‘áº¿n káº¿t quáº£**

---

## 3.4. Sá»± ra Ä‘á»i cá»§a cÃ¡c biáº¿n thá»ƒ SHAP  

Khi mÃ´ hÃ¬nh vÃ  dá»¯ liá»‡u ngÃ y cÃ ng lá»›n, viá»‡c tÃ­nh toÃ¡n **Shapley Values** Ä‘áº§y Ä‘á»§ trá»Ÿ nÃªn tá»‘n kÃ©m.  
Do Ä‘Ã³, nhiá»u **biáº¿n thá»ƒ má»Ÿ rá»™ng cá»§a SHAP** nhÆ° **TreeSHAP**, **KernelSHAP**, **DeepSHAP** Ä‘Ã£ Ä‘Æ°á»£c phÃ¡t triá»ƒn Ä‘á»ƒ cÃ¢n báº±ng giá»¯a **tá»‘c Ä‘á»™** vÃ  **Ä‘á»™ chÃ­nh xÃ¡c**.  
CÃ¡c phÆ°Æ¡ng phÃ¡p nÃ y sáº½ Ä‘Æ°á»£c trÃ¬nh bÃ y chi tiáº¿t hÆ¡n á»Ÿ **pháº§n 6** cá»§a blog.


# 4. VÃ¬ sao SHAP khÃ´ng thá»ƒ trá»±c tiáº¿p giáº£i thÃ­ch Genetic Algorithm â€“ vÃ  nhá»¯ng hÆ°á»›ng kháº¯c phá»¥c kháº£ thi

## 4.1. Váº¥n Ä‘á» â€œHá»™p Ä‘en trong há»™p Ä‘enâ€ cá»§a trÃ­ tuá»‡ tiáº¿n hoÃ¡
Trong há»c mÃ¡y hiá»‡n Ä‘áº¡i, kháº£ nÄƒng giáº£i thÃ­ch mÃ´ hÃ¬nh (model interpretability) lÃ  yáº¿u tá»‘ thiáº¿t yáº¿u Ä‘á»ƒ Ä‘áº£m báº£o tÃ­nh tin cáº­y, minh báº¡ch vÃ  kháº£ nÄƒng kiá»ƒm chá»©ng cá»§a há»‡ thá»‘ng AI.
CÃ¡c mÃ´ hÃ¬nh cÃ³ Ä‘á»™ phá»©c táº¡p cao nhÆ° máº¡ng nÆ¡-ron sÃ¢u, mÃ´ hÃ¬nh boosting, hoáº·c cÃ¡c thuáº­t toÃ¡n tiáº¿n hoÃ¡ thÆ°á»ng Ä‘áº¡t hiá»‡u nÄƒng vÆ°á»£t trá»™i, song láº¡i hoáº¡t Ä‘á»™ng nhÆ° nhá»¯ng â€œhá»™p Ä‘enâ€ â€“ con ngÆ°á»i khÃ³ hiá»ƒu Ä‘Æ°á»£c táº¡i sao vÃ  báº±ng cÃ¡ch nÃ o chÃºng Ä‘Æ°a ra káº¿t quáº£.

Äá»ƒ Ä‘á»‘i phÃ³ vá»›i váº¥n Ä‘á» nÃ y, cÃ¡c ká»¹ thuáº­t Giáº£i thÃ­ch mÃ´ hÃ¬nh (XAI â€“ eXplainable Artificial Intelligence) nhÆ° LIME, Permutation Importance, vÃ  Ä‘áº·c biá»‡t SHAP (SHapley Additive exPlanations) Ä‘Ã£ Ä‘Æ°á»£c phÃ¡t triá»ƒn. SHAP ná»•i báº­t bá»Ÿi ná»n táº£ng toÃ¡n há»c vá»¯ng cháº¯c tá»« lÃ½ thuyáº¿t giÃ¡ trá»‹ Shapley trong trÃ² chÆ¡i há»£p tÃ¡c, cho phÃ©p Ä‘á»‹nh lÆ°á»£ng má»©c Ä‘Ã³ng gÃ³p trung bÃ¬nh cá»§a tá»«ng Ä‘áº·c trÆ°ng tá»›i Ä‘áº§u ra mÃ´ hÃ¬nh.

Tuy nhiÃªn, khi chuyá»ƒn sang lÄ©nh vá»±c tÃ­nh toÃ¡n tiáº¿n hoÃ¡ (Evolutionary Computation) â€“ cá»¥ thá»ƒ lÃ  Genetic Algorithm (GA) â€“ cÃ¢u há»i náº£y sinh:

Liá»‡u SHAP cÃ³ thá»ƒ giÃºp giáº£i thÃ­ch â€œcÃ¡ch thá»©c tiáº¿n hoÃ¡â€ cá»§a GA giá»‘ng nhÆ° cÃ¡ch nÃ³ giáº£i thÃ­ch cÃ¡c mÃ´ hÃ¬nh há»c mÃ¡y khÃ¡c khÃ´ng?

CÃ¢u tráº£ lá»i, Ã­t nháº¥t á»Ÿ thá»i Ä‘iá»ƒm hiá»‡n táº¡i, lÃ  khÃ´ng thá»ƒ trá»±c tiáº¿p.

## 4.2. GA vÃ  SHAP: Hai tháº¿ giá»›i khÃ¡c nhau

### 4.2.1. Genetic Algorithm (GA)

GA lÃ  má»™t thuáº­t toÃ¡n tá»‘i Æ°u hoÃ¡ khÃ´ng dá»±a trÃªn gradient, mÃ´ phá»ng quÃ¡ trÃ¬nh tiáº¿n hoÃ¡ tá»± nhiÃªn. Äiá»ƒm cá»‘t lÃµi lÃ  Genetic Algorithm (GA) khÃ´ng pháº£i má»™t mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n (predictive model), mÃ  lÃ  má»™t quy trÃ¬nh **tá»‘i Æ°u hoÃ¡ Ä‘á»™ng** (dynamic optimization process) â€” tá»©c lÃ  thay vÃ¬ há»c má»™t hÃ m Ã¡nh xáº¡ cá»‘ Ä‘á»‹nh giá»¯a Ä‘áº§u vÃ o vÃ  Ä‘áº§u ra, GA liÃªn tá»¥c tiáº¿n hoÃ¡ quáº§n thá»ƒ nghiá»‡m thÃ´ng qua cÃ¡c bÆ°á»›c **chá»n lá»c**, **lai ghÃ©p** vÃ  **Ä‘á»™t biáº¿n** Ä‘á»ƒ dáº§n tÃ¬m ra nghiá»‡m tá»‘t nháº¥t.

### 4.2.2.  SHAP

Má»¥c tiÃªu cá»§a SHAP lÃ  Ä‘á»‹nh lÆ°á»£ng má»©c Ä‘á»™ Ä‘Ã³ng gÃ³p cá»§a tá»«ng Ä‘áº·c trÆ°ng Ä‘áº§u vÃ o Ä‘á»‘i vá»›i káº¿t quáº£ Ä‘áº§u ra cá»§a má»™t **mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n tÄ©nh** â€” tá»©c lÃ  má»™t hÃ m Ã¡nh xáº¡ xÃ¡c Ä‘á»‹nh tá»« khÃ´ng gian Ä‘áº§u vÃ o sang Ä‘áº§u ra, kÃ½ hiá»‡u chung lÃ :

$$
f: X \rightarrow Y
$$
SHAP hoáº¡t Ä‘á»™ng dá»±a trÃªn **nguyÃªn táº¯c additivity** (tÃ­nh cá»™ng gá»™p), tá»©c lÃ  Ä‘áº§u ra cá»§a mÃ´ hÃ¬nh cÃ³ thá»ƒ Ä‘Æ°á»£c biá»ƒu diá»…n nhÆ° tá»•ng cÃ¡c Ä‘Ã³ng gÃ³p cá»§a tá»«ng Ä‘áº·c trÆ°ng, cÃ¹ng vá»›i má»™t giÃ¡ trá»‹ ná»n (baseline prediction):

$$
f(x) = \phi_0 + \sum_i \phi_i
$$

Äiá»ƒm cá»‘t lÃµi cá»§a SHAP náº±m á»Ÿ viá»‡c mÃ´ hÃ¬nh pháº£i **cÃ³ thá»ƒ Ä‘Æ°á»£c gá»i láº·p láº¡i nhiá»u láº§n** vá»›i cÃ¡c tá»• há»£p Ä‘áº·c trÆ°ng khÃ¡c nhau, Ä‘á»ƒ Æ°á»›c lÆ°á»£ng **tÃ¡c Ä‘á»™ng biÃªn (marginal contribution)** cá»§a tá»«ng Ä‘áº·c trÆ°ng lÃªn káº¿t quáº£ dá»± Ä‘oÃ¡n. Khi Ä‘Ã³, SHAP coi má»—i Ä‘áº·c trÆ°ng nhÆ° má»™t â€œngÆ°á»i chÆ¡iâ€ trong trÃ² chÆ¡i há»£p tÃ¡c, vÃ  phÃ¢n phá»‘i **cÃ´ng báº±ng giÃ¡ trá»‹ dá»± Ä‘oÃ¡n** giá»¯a cÃ¡c ngÆ°á»i chÆ¡i theo nguyÃªn táº¯c Shapley.

Vá» máº·t báº£n cháº¥t, SHAP lÃ  phÆ°Æ¡ng phÃ¡p **giáº£i thÃ­ch hÃ m (function-based explanation)**, chá»© khÃ´ng pháº£i **quy trÃ¬nh (process-based explanation)**. NÃ³ yÃªu cáº§u mÃ´ hÃ¬nh cÃ³:

- **Äáº§u vÃ o â€“ Ä‘áº§u ra xÃ¡c Ä‘á»‹nh**,  
- **TÃ­nh láº·p láº¡i (determinism)** trong dá»± Ä‘oÃ¡n,  
- **Kháº£ nÄƒng tÃ¡ch biá»‡t rÃµ rÃ ng giá»¯a cÃ¡c Ä‘áº·c trÆ°ng**.

Do Ä‘Ã³, SHAP phÃ¹ há»£p Ä‘á»ƒ giáº£i thÃ­ch cÃ¡c mÃ´ hÃ¬nh nhÆ° há»“i quy, máº¡ng nÆ¡-ron, hay cÃ¡c mÃ´ hÃ¬nh tree-based (nhÆ° Random Forest, XGBoost) â€” nÆ¡i ta cÃ³ thá»ƒ xÃ¡c Ä‘á»‹nh rÃµ rÃ ng $f(x)$ cho má»—i Ä‘iá»ƒm dá»¯ liá»‡u.

NgÆ°á»£c láº¡i, vá»›i cÃ¡c thuáº­t toÃ¡n nhÆ° GA, nÆ¡i khÃ´ng tá»“n táº¡i má»™t **hÃ m dá»± Ä‘oÃ¡n tÄ©nh duy nháº¥t**, mÃ  chá»‰ cÃ³ má»™t **quy trÃ¬nh tiáº¿n hoÃ¡ phá»¥ thuá»™c vÃ o ngáº«u nhiÃªn vÃ  thá»i gian**, SHAP khÃ´ng thá»ƒ Ã¡p dá»¥ng trá»±c tiáº¿p.

## 4.3. Táº¡i sao SHAP khÃ´ng thá»ƒ giáº£i thÃ­ch GA trá»±c tiáº¿p

SHAP yÃªu cáº§u mÃ´ hÃ¬nh cÃ³ **hÃ m dá»± Ä‘oÃ¡n cá»‘ Ä‘á»‹nh** $f(x)$ cÃ³ thá»ƒ gá»i láº¡i nhiá»u láº§n Ä‘á»ƒ Ä‘o tÃ¡c Ä‘á»™ng cá»§a tá»«ng Ä‘áº·c trÆ°ng. GA khÃ´ng thá»a Ä‘iá»u kiá»‡n nÃ y vÃ¬:  

### 4.3.1. GA khÃ´ng cÃ³ hÃ m mÃ´ hÃ¬nh cá»‘ Ä‘á»‹nh
GA chá»‰ biáº¿t **hÃ m fitness**, cÃ²n quÃ¡ trÃ¬nh tiáº¿n hoÃ¡ lÃ  tá»• há»£p nhiá»u phÃ©p biáº¿n Ä‘á»•i ngáº«u nhiÃªn:

$$
\text{Population}_{t+1} = \text{Mutate}(\text{Crossover}(\text{Select}(\text{Population}_t)))
$$

Káº¿t quáº£ phá»¥ thuá»™c vÃ o:  
- Cáº¥u hÃ¬nh khá»Ÿi táº¡o  
- Ngáº«u nhiÃªn trong lai ghÃ©p/Ä‘á»™t biáº¿n  
- Thá»© tá»± Ä‘Ã¡nh giÃ¡ fitness  

â†’ Cháº¡y láº¡i GA vá»›i cÃ¹ng tham sá»‘ váº«n cÃ³ thá»ƒ cho káº¿t quáº£ khÃ¡c nhau. **KhÃ´ng tá»“n táº¡i hÃ m $f(x)$ cá»‘ Ä‘á»‹nh** Ä‘á»ƒ SHAP Ã¡p dá»¥ng.

### 4.3.2. GA khÃ´ng táº¡o ra Ä‘áº§u ra tá»©c thá»i
Äá»ƒ Ä‘Ã¡nh giÃ¡ áº£nh hÆ°á»Ÿng cá»§a má»™t gene, pháº£i tiáº¿n hoÃ¡ láº¡i toÃ n bá»™ quÃ¡ trÃ¬nh, vÃ¬ loáº¡i bá» gene sáº½ thay Ä‘á»•i toÃ n bá»™ quá»¹ Ä‘áº¡o tiáº¿n hoÃ¡.

### 4.3.3. GA lÃ  quy trÃ¬nh tuáº§n tá»± â€“ ngáº«u nhiÃªn â€“ phi tuyáº¿n
GA lÃ  phi tuyáº¿n, phi xÃ¡c Ä‘á»‹nh, vá»›i tÆ°Æ¡ng tÃ¡c máº¡nh giá»¯a cÃ¡c gene. Viá»‡c tÃ­nh SHAP value trá»±c tiáº¿p sáº½ chá»‰ táº¡o ra **nhiá»…u thá»‘ng kÃª**, khÃ´ng pháº£n Ã¡nh báº£n cháº¥t tiáº¿n hoÃ¡.

> NhÆ° váº­y, nguyÃªn nhÃ¢n khiáº¿n SHAP khÃ´ng thá»ƒ Ã¡p dá»¥ng trá»±c tiáº¿p cho GA lÃ  **tÃ­nh ngáº«u nhiÃªn, phi tuyáº¿n vÃ  thiáº¿u hÃ m dá»± Ä‘oÃ¡n cá»‘ Ä‘á»‹nh**. Äiá»u nÃ y dáº«n Ä‘áº¿n sá»± khÃ¡c biá»‡t rÃµ rÃ ng trong báº£n cháº¥t â€œhá»™p Ä‘enâ€ cá»§a GA so vá»›i cÃ¡c mÃ´ hÃ¬nh tree-based, nhÆ° sáº½ trÃ¬nh bÃ y á»Ÿ pháº§n tiáº¿p theo.

## 4.4. Sá»± khÃ¡c biá»‡t vá» báº£n cháº¥t â€œhá»™p Ä‘enâ€ giá»¯a GA vÃ  cÃ¡c mÃ´ hÃ¬nh tree-based

Máº·c dÃ¹ cáº£ GA vÃ  cÃ¡c mÃ´ hÃ¬nh tree-based (nhÆ° Random Forest, Gradient Boosting, XGBoost) thÆ°á»ng Ä‘Æ°á»£c xem lÃ  â€œhá»™p Ä‘enâ€ trong há»c mÃ¡y, song báº£n cháº¥t â€œÄ‘enâ€ cá»§a hai nhÃ³m nÃ y Ä‘áº¿n tá»« nhá»¯ng nguyÃªn nhÃ¢n hoÃ n toÃ n khÃ¡c nhau.

Thá»© nháº¥t, Ä‘á»‘i vá»›i mÃ´ hÃ¬nh tree-based, â€œhá»™p Ä‘enâ€ xuáº¥t phÃ¡t tá»« Ä‘á»™ phá»©c táº¡p cáº¥u trÃºc vÃ  sá»‘ lÆ°á»£ng mÃ´ hÃ¬nh con.  
Má»™t mÃ´ hÃ¬nh nhÆ° Random Forest cÃ³ thá»ƒ bao gá»“m hÃ ng trÄƒm Ä‘áº¿n hÃ ng nghÃ¬n cÃ¢y quyáº¿t Ä‘á»‹nh, má»—i cÃ¢y há»c má»™t táº­p con dá»¯ liá»‡u vÃ  Ä‘áº·c trÆ°ng khÃ¡c nhau. Khi dá»± Ä‘oÃ¡n, Ä‘áº§u ra cá»§a tá»«ng cÃ¢y Ä‘Æ°á»£c káº¿t há»£p (thÃ´ng qua trung bÃ¬nh hoáº·c voting), táº¡o thÃ nh má»™t há»‡ thá»‘ng phi tuyáº¿n, khÃ³ suy luáº­n trá»±c tiáº¿p má»‘i quan há»‡ giá»¯a tá»«ng Ä‘áº·c trÆ°ng vÃ  Ä‘áº§u ra cuá»‘i cÃ¹ng.  
Tuy nhiÃªn, vá» báº£n cháº¥t, cÃ¡c mÃ´ hÃ¬nh tree-based váº«n lÃ  **hÃ m xÃ¡c Ä‘á»‹nh** $f(x)$: vá»›i cÃ¹ng má»™t Ä‘áº§u vÃ o, luÃ´n cho ra cÃ¹ng má»™t Ä‘áº§u ra. Äiá»u nÃ y giÃºp chÃºng cÃ³ thá»ƒ Ä‘Æ°á»£c giáº£i thÃ­ch **háº­u Ä‘á»‹nh (post-hoc)** báº±ng SHAP hoáº·c cÃ¡c ká»¹ thuáº­t tÆ°Æ¡ng tá»±.

Thá»© hai, Ä‘á»‘i vá»›i GA, tÃ­nh â€œhá»™p Ä‘enâ€ láº¡i Ä‘áº¿n tá»« báº£n cháº¥t **quÃ¡ trÃ¬nh tiáº¿n hoÃ¡ ngáº«u nhiÃªn vÃ  Ä‘á»™ng theo thá»i gian**.  
GA khÃ´ng há»c má»™t hÃ m cá»‘ Ä‘á»‹nh, mÃ  thá»±c hiá»‡n quÃ¡ trÃ¬nh láº·p gá»“m chá»n lá»c, lai ghÃ©p vÃ  Ä‘á»™t biáº¿n â€” liÃªn tá»¥c thay Ä‘á»•i khÃ´ng gian nghiá»‡m qua tá»«ng tháº¿ há»‡. Káº¿t quáº£ cuá»‘i cÃ¹ng (nghiá»‡m tá»‘i Æ°u) khÃ´ng chá»‰ phá»¥ thuá»™c vÃ o dá»¯ liá»‡u Ä‘áº§u vÃ o mÃ  cÃ²n vÃ o tráº¡ng thÃ¡i khá»Ÿi táº¡o, cÃ¡c tham sá»‘ tiáº¿n hoÃ¡ (nhÆ° mutation rate, crossover rate) vÃ  yáº¿u tá»‘ ngáº«u nhiÃªn trong tá»«ng láº§n cháº¡y.  
Do Ä‘Ã³, GA khÃ´ng cÃ³ **hÃ m Ã¡nh xáº¡ tÄ©nh** tá»« Ä‘áº§u vÃ o sang Ä‘áº§u ra, mÃ  chá»‰ lÃ  má»™t **chuá»—i biáº¿n Ä‘á»•i phi xÃ¡c Ä‘á»‹nh**. Äiá»u nÃ y khiáº¿n viá»‡c Ã¡p dá»¥ng SHAP hay cÃ¡c phÆ°Æ¡ng phÃ¡p dá»±a trÃªn khÃ¡i niá»‡m â€œÄ‘Ã³ng gÃ³p Ä‘áº·c trÆ°ngâ€ trá»Ÿ nÃªn khÃ´ng kháº£ thi theo nghÄ©a truyá»n thá»‘ng.

TÃ³m láº¡i, â€œtÃ­nh há»™p Ä‘enâ€ cá»§a tree-based models Ä‘áº¿n tá»« sá»± phá»©c táº¡p cáº¥u trÃºc nhÆ°ng váº«n cÃ³ thá»ƒ giáº£i thÃ­ch háº­u Ä‘á»‹nh, trong khi â€œtÃ­nh há»™p Ä‘enâ€ cá»§a GA Ä‘áº¿nn tá»« sá»± **ngáº«u nhiÃªn, phi tuyáº¿n vÃ  khÃ´ng cÃ³ hÃ m mÃ´ hÃ¬nh cá»‘ Ä‘á»‹nh** â€” khiáº¿n viá»‡c giáº£i thÃ­ch cáº§n hÆ°á»›ng sang **má»©c meta** hoáº·c **surrogate model** thay vÃ¬ trá»±c tiáº¿p.

## 4.5. CÃ¡c hÆ°á»›ng giáº£i thÃ­ch giÃ¡n tiáº¿p cho GA (Indirect explainability for GA)

### Ã tÆ°á»Ÿng chung

VÃ¬ GA lÃ  má»™t quy trÃ¬nh tiáº¿n hÃ³a Ä‘á»™ng (khÃ´ng pháº£i má»™t hÃ m dá»± Ä‘oÃ¡n tÄ©nh), nÃªn SHAP khÃ´ng thá»ƒ Ã¡p dá»¥ng trá»±c tiáº¿p Ä‘á»ƒ â€œgiáº£i thÃ­châ€ GA. Tuy nhiÃªn, ta cÃ³ thá»ƒ giáº£i thÃ­ch káº¿t quáº£ hoáº·c hÃ nh vi cá»§a GA báº±ng cÃ¡c phÆ°Æ¡ng phÃ¡p giÃ¡n tiáº¿p sau â€” má»—i hÆ°á»›ng Ä‘á»u cho phÃ©p Ä‘Æ°a vÃ o SHAP hoáº·c quan niá»‡m Shapley Ä‘á»ƒ Ä‘áº¡t interpretability.

---

### 4.5.1. Surrogate-based explainability (MÃ´ hÃ¬nh thay tháº¿ + SHAP)

**CÆ¡ cháº¿:**

Trong quÃ¡ trÃ¬nh cháº¡y GA, lÆ°u láº¡i nhiá»u cáº·p (genome, fitness) (vÃ­ dá»¥: cÃ¡c cÃ¡ thá»ƒ trong nhiá»u tháº¿ há»‡).  

Huáº¥n luyá»‡n má»™t mÃ´ hÃ¬nh surrogate $\hat{f}$ (vÃ­ dá»¥ XGBoost, RandomForest) Ä‘á»ƒ xáº¥p xá»‰ hÃ m fitness:  
$$
\text{fitness} \approx \hat{f}(\text{genome})
$$

Ãp dá»¥ng SHAP lÃªn $\hat{f}$ Ä‘á»ƒ tÃ­nh SHAP values cho tá»«ng gene â€” tá»« Ä‘Ã³ biáº¿t gene nÃ o áº£nh hÆ°á»Ÿng máº¡nh Ä‘áº¿n fitness.

**Code minh há»a:**

```python
import numpy as np
import shap
from sklearn.ensemble import RandomForestRegressor

# Giáº£ sá»­ ta Ä‘Ã£ cÃ³ dá»¯ liá»‡u GA log
genomes = np.random.rand(200, 10)
fitness = np.sin(genomes[:, 0]) + np.cos(genomes[:, 1]) + np.random.randn(200) * 0.1

# Huáº¥n luyá»‡n mÃ´ hÃ¬nh surrogate
model = RandomForestRegressor()
model.fit(genomes, fitness)

# Ãp dá»¥ng SHAP
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(genomes)

# Hiá»ƒn thá»‹ káº¿t quáº£
shap.summary_plot(shap_values, genomes, feature_names=[f"gene_{i}" for i in range(genomes.shape[1])])
```

![SHAP summary plot](images/shap_summary.png)


### 4.5.2. Logging + Post-hoc Attribution (Ghi log tiáº¿n hoÃ¡ â†’ phÃ¢n tÃ­ch Shapley)

#### **CÆ¡ cháº¿**

- Ghi chÃ©p chi tiáº¿t **logs** trong quÃ¡ trÃ¬nh tiáº¿n hoÃ¡:  
  - CÃ¡ thá»ƒ Ä‘Æ°á»£c chá»n hoáº·c loáº¡i bá».  
  - **Marginal contribution** cá»§a cÃ¡ thá»ƒ hoáº·c biáº¿n trong tá»«ng tháº¿ há»‡.  
  - **Táº§n suáº¥t xuáº¥t hiá»‡n gene**, hoáº·c cÃ¡c thá»‘ng kÃª vá» sá»± thay Ä‘á»•i fitness.  

- DÃ¹ng cÃ¡c **chá»‰ sá»‘ thá»‘ng kÃª** (vÃ­ dá»¥: táº§n suáº¥t, *average marginal gain*, *rate of improvement* khi gene xuáº¥t hiá»‡n) Ä‘á»ƒ **Ä‘á»‹nh lÆ°á»£ng táº§m quan trá»ng cá»§a gene**.  

- Ãp cÃ¡c **biáº¿n thá»‘ng kÃª** nÃ y lÃ m **input cho SHAP** (hoáº·c chá»‰ bÃ¡o *Shapley-like*) nháº±m **giáº£i thÃ­ch táº¡i sao GA tiáº¿n hoÃ¡ theo hÆ°á»›ng Ä‘Ã³**.

---

#### **Dáº«n chá»©ng há»c thuáº­t**

CÃ¡c cÃ´ng trÃ¬nh **game-theoretic feature selection** (*Zaeri-Amirani et al., 2018*) Ä‘Ã£ minh hoáº¡ cÃ¡ch **GA chá»n cÃ¡c coalition** vÃ  **tÃ­nh marginal contribution** Ä‘á»ƒ **Æ°á»›c lÆ°á»£ng giÃ¡ trá»‹ Shapley**.  
â†’ Ã tÆ°á»Ÿng **logging coalition + láº¥y marginal** chÃ­nh lÃ  tiá»n Ä‘á» cho hÆ°á»›ng tiáº¿p cáº­n nÃ y.  

ğŸ“„ *Tham kháº£o:* Zaeri-Amirani, et al. (2018). *GA-based Monte Carlo Shapley approximation for feature selection.*

---

#### **Ã nghÄ©a / Æ¯u nhÆ°á»£c**

- **Æ¯u Ä‘iá»ƒm:**  
  - Giá»¯ láº¡i pháº§n **Ä‘á»™ng há»c tiáº¿n hoÃ¡** â€” cho phÃ©p phÃ¢n tÃ­ch chiáº¿n lÆ°á»£c thÃ­ch nghi qua cÃ¡c giai Ä‘oáº¡n.  
  - CÃ³ thá»ƒ chá»‰ ra **gene nÃ o quan trá»ng á»Ÿ tá»«ng giai Ä‘oáº¡n** (Ä‘áº§u, giá»¯a, cuá»‘i).  

- **NhÆ°á»£c Ä‘iá»ƒm:**  
  - Cáº§n **lÆ°u lÆ°á»£ng dá»¯ liá»‡u lá»›n** (toÃ n bá»™ lá»‹ch sá»­ tiáº¿n hoÃ¡).  
  - **PhÃ¢n tÃ­ch phá»©c táº¡p** vÃ¬ mang tÃ­nh chuá»—i thá»i gian (*time-series of importance*).  

---

#### **VÃ­ dá»¥ minh hoáº¡ (Python pseudo-code)**

```python
# Logging GA evolution process
import pandas as pd

logs = []

for gen in range(num_generations):
    population = evolve(population)
    for ind in population:
        logs.append({
            'generation': gen,
            'individual': ind.genes,
            'fitness': ind.fitness,
            'marginal_gain': ind.fitness - avg_prev_fitness(ind)
        })

# Convert logs to DataFrame
df_logs = pd.DataFrame(logs)

# Aggregate statistics per gene
gene_stats = df_logs.groupby('gene').agg({
    'marginal_gain': 'mean',
    'fitness': 'mean'
}).reset_index()

# Apply SHAP (post-hoc explainability)
import shap
model = shap.Explainer(surrogate_model)
shap_values = model(gene_stats)

shap.summary_plot(shap_values, gene_stats)
```
![SHAP summary plot](images/post_hoc.png)
```
   generation                                        gene_values   fitness  \
0           0  [0.9255534584552242, 0.21746386134556728, 0.13...  2.791049   
1           0  [0.6129517726409283, 0.708922426261847, 0.0337...  1.135790   
2           0  [0.9304056014269897, 0.7445169972848723, 0.524...  3.474486   
3           0  [0.3835035901648618, 0.5292369439072145, 0.792...  1.888319   
4           0  [0.06554067151333975, 0.9469961975270887, 0.55...  0.908424   

   marginal_gain  
0       0.558826  
1      -1.096434  
2       1.242262  
3      -0.343905  
4      -1.323799  
```
### 4.5.3. SHAP-guided diagnostics â€” DÃ¹ng SHAP Ä‘á»ƒ cháº©n Ä‘oÃ¡n GA

#### CÆ¡ cháº¿

Ã tÆ°á»Ÿng cá»§a hÆ°á»›ng nÃ y lÃ  **Ã¡p dá»¥ng SHAP khÃ´ng Ä‘á»ƒ giáº£i thÃ­ch báº£n thÃ¢n GA**, mÃ  Ä‘á»ƒ **giÃ¡m sÃ¡t hoáº·c cháº©n Ä‘oÃ¡n hÃ nh vi tiáº¿n hoÃ¡ cá»§a GA** thÃ´ng qua cÃ¡c mÃ´ hÃ¬nh phá»¥ (surrogate) hoáº·c táº­p nghiá»‡m tá»‘t nháº¥t (best solutions) mÃ  GA sinh ra.

Cá»¥ thá»ƒ:

- Ãp SHAP lÃªn **mÃ´ hÃ¬nh surrogate** Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»ƒ xáº¥p xá»‰ má»‘i quan há»‡ giá»¯a bá»™ gene vÃ  fitness,  
  **hoáº·c** trá»±c tiáº¿p lÃªn **táº­p nghiá»‡m tá»‘t nháº¥t** thu Ä‘Æ°á»£c sau quÃ¡ trÃ¬nh tiáº¿n hoÃ¡.  
- Náº¿u káº¿t quáº£ SHAP cho tháº¥y **fitness bá»‹ chi phá»‘i bá»Ÿi má»™t sá»‘ Ã­t gene**, Ä‘iá»u nÃ y gá»£i Ã½ hiá»‡n tÆ°á»£ng:
  - *loss of diversity* (máº¥t Ä‘a dáº¡ng quáº§n thá»ƒ), hoáº·c  
  - *bias* do GA há»™i tá»¥ sá»›m vÃ o má»™t **local optimum**.  
- Khi Ä‘Ã³, ngÆ°á»i nghiÃªn cá»©u cÃ³ thá»ƒ Ä‘iá»u chá»‰nh **siÃªu tham sá»‘ cá»§a GA** Ä‘á»ƒ kháº¯c phá»¥c:
  - tÄƒng mutation rate  
  - thay Ä‘á»•i selection pressure  
  - bá»• sung cÆ¡ cháº¿ duy trÃ¬ Ä‘a dáº¡ng (niching, crowding, speciation)

---
#### CÆ¡ sá»Ÿ há»c thuáº­t vÃ  minh hoáº¡

Fryer et al. (2021) trong bÃ i viáº¿t vá» **giá»›i háº¡n cá»§a giÃ¡ trá»‹ Shapley cho bÃ i toÃ¡n chá»n Ä‘áº·c trÆ°ng (feature selection)** Ä‘Ã£ chá»‰ ra ráº±ng cÃ¡c chá»‰ sá»‘ attribution nhÆ° SHAP cÃ³ thá»ƒ phÃ¡t hiá»‡n nhá»¯ng Ä‘áº·c trÆ°ng â€œÃ¡p Ä‘áº£oâ€, tá»« Ä‘Ã³ cáº£nh bÃ¡o vá» **sá»± máº¥t cÃ¢n báº±ng Ä‘Ã³ng gÃ³p** trong mÃ´ hÃ¬nh.  
Ã tÆ°á»Ÿng nÃ y cÃ³ thá»ƒ má»Ÿ rá»™ng cho GA: náº¿u má»™t vÃ i gene cÃ³ attribution vÆ°á»£t trá»™i, ta cÃ³ thá»ƒ coi Ä‘Ã³ lÃ  tÃ­n hiá»‡u cáº£nh bÃ¡o vá» **thiÃªn lá»‡ch tiáº¿n hoÃ¡ (evolutionary bias)**.

Thá»±c tiá»…n, viá»‡c Ã¡p dá»¥ng SHAP-guided diagnostics giÃºp theo dÃµi **Ä‘á»™ng lá»±c há»c thÃ­ch nghi** (adaptive dynamics) cá»§a GA vÃ  Ä‘iá»u chá»‰nh cáº¥u hÃ¬nh Ä‘á»ƒ trÃ¡nh há»™i tá»¥ sá»›m.

---

#### Ã nghÄ©a vÃ  Ä‘Ã¡nh giÃ¡

**Æ¯u Ä‘iá»ƒm:**
- SHAP trá»Ÿ thÃ nh cÃ´ng cá»¥ **giÃ¡m sÃ¡t** (monitoring) tiáº¿n trÃ¬nh tiáº¿n hoÃ¡ cá»§a GA.  
- Dá»… triá»ƒn khai, cÃ³ thá»ƒ káº¿t há»£p vá»›i cÃ¡c cÃ´ng cá»¥ trá»±c quan hoÃ¡ (vÃ­ dá»¥ summary plot hoáº·c dependence plot).

**Háº¡n cháº¿:**
- PhÆ°Æ¡ng phÃ¡p váº«n **giÃ¡n tiáº¿p** â€” khÃ´ng phÃ¢n tÃ­ch GA á»Ÿ cáº¥p Ä‘á»™ cÆ¡ cháº¿ mÃ  thÃ´ng qua mÃ´ hÃ¬nh trung gian.  
- Viá»‡c Ä‘iá»u chá»‰nh GA dá»±a quÃ¡ nhiá»u vÃ o attribution cÃ³ thá»ƒ lÃ m **giáº£m tÃ­nh khÃ¡m phÃ¡** náº¿u can thiá»‡p quÃ¡ má»©c vÃ o mutation hoáº·c selection.

---

**Nguá»“n tham kháº£o:**  
Fryer et al., *On the Limitations of Shapley Values for Feature Selection*, arXiv:2106.XXXX, 2021.

### 4.5.4. HÆ°á»›ng ngÆ°á»£c láº¡i â€” GA â†’ Shapley (Æ°á»›c lÆ°á»£ng giÃ¡ trá»‹ Shapley)

ÄÃ¢y lÃ  hÆ°á»›ng ná»•i báº­t trong tÃ i liá»‡u hiá»‡n cÃ³ vÃ  lÃ  máº¥u chá»‘t nÃªn nháº¥n máº¡nh trong blog â€” cÃ³ cÆ¡ sá»Ÿ há»c thuáº­t rÃµ rÃ ng (Zaeri-Amirani et al., 2018) vÃ  má»™t sá»‘ cÃ´ng trÃ¬nh gáº§n Ä‘Ã¢y má»Ÿ rá»™ng Ã½ tÆ°á»Ÿng nÃ y.

#### a. Táº¡i sao cáº§n Æ°á»›c lÆ°á»£ng Shapley?

**Váº¥n Ä‘á»:**  
Shapley value theo lÃ½ thuyáº¿t yÃªu cáº§u xÃ©t táº¥t cáº£ cÃ¡c táº­p con â†’ chi phÃ­ tÃ­nh toÃ¡n O(2^N) (khÃ´ng kháº£ thi khi N lá»›n).

**Giáº£i phÃ¡p tá»•ng quÃ¡t:**  
- Sampling (Monte-Carlo)  
- Kernel-approximation (KernelSHAP)  
- Heuristic chá»n máº«u thÃ´ng minh  

GA lÃ  má»™t lá»±a chá»n sampling hÆ°á»›ng má»¥c tiÃªu (targeted sampling) Ä‘á»ƒ tÃ¬m nhá»¯ng coalition cÃ³ Ä‘Ã³ng gÃ³p lá»›n, tá»« Ä‘Ã³ Æ°á»›c lÆ°á»£ng giÃ¡ trá»‹ trung bÃ¬nh chÃ­nh xÃ¡c hÆ¡n vá»›i Ã­t máº«u hÆ¡n.

**Nguá»“n gá»‘c & dáº«n chá»©ng:**  
Zaeri-Amirani et al. (2018) trÃ¬nh bÃ y GA-based Monte-Carlo method Ä‘á»ƒ sinh cÃ¡c coalition cÃ³ marginal contribution lá»›n vÃ  Æ°á»›c lÆ°á»£ng Shapley value tá»« chÃºng; phÆ°Æ¡ng phÃ¡p nÃ y giáº£m phá»©c táº¡p tÃ­nh toÃ¡n vÃ  Ä‘áº¡t káº¿t quáº£ tá»‘t trong bÃ i toÃ¡n giáº£m bÃ¡o Ä‘á»™ng giáº£ trÃªn dá»¯ liá»‡u PhysioNet (AUC â‰ˆ 0.81 cho Shapley Âµ=3.5).

---

#### b. NguyÃªn lÃ½ hoáº¡t Ä‘á»™ng (chi tiáº¿t thuáº­t toÃ¡n)

**MÃ£ hÃ³a (Encoding):**  
Má»—i chromosome = vector nhá»‹ phÃ¢n biá»ƒu diá»…n coalition (vÃ­ dá»¥ cÃ¡c feature cÃ³ trong coalition).

**Fitness function:**  
Äá»‹nh nghÄ©a marginal contribution cá»§a má»™t feature i Ä‘á»‘i vá»›i coalition T:

$$
f_i(\text{chr}(T)) = \nu(T \cup \{i\}) - \nu(T)
$$

Trong Ä‘Ã³ Î½(T) lÃ  giÃ¡ trá»‹ (score) cá»§a coalition T (Zaeri-Amirani dÃ¹ng káº¿t há»£p sensitivity & specificity lÃ m Î½).

**QuÃ¡ trÃ¬nh tiáº¿n hoÃ¡:**  
1. Khá»Ÿi táº¡o population cho má»—i kÃ­ch thÆ°á»›c coalition t  
2. Tiáº¿n hoÃ¡ báº±ng selection / crossover / mutation Ä‘á»ƒ tÃ¬m cÃ¡c coalition cÃ³ marginal contribution lá»›n  
3. Láº·p cho nhiá»u t vÃ  bÃ¬nh quÃ¢n cÃ¡c marginal contributions Ä‘á»ƒ Æ°á»›c lÆ°á»£ng Ï†Ì‚_i

---

#### c. LÃ½ luáº­n thá»‘ng kÃª & phá»©c táº¡p

- Náº¿u giá»›i háº¡n kÃ­ch thÆ°á»›c coalition tá»›i n_max vÃ  sinh n_G máº«u, complexity giáº£m tá»« exponential xuá»‘ng:

$$
O(n_f \times n_{\text{max}} \times n_G)
$$

- Sá»­ dá»¥ng mÃ´ hÃ¬nh phÃ¢n phá»‘i cá»±c trá»‹ (Gumbel / EX1) Ä‘á»ƒ Æ°á»›c lÆ°á»£ng expected max cá»§a marginal sample vÃ  Ä‘iá»u chá»‰nh sai sá»‘ sampling.

---

#### d. Káº¿t quáº£ thá»±c nghiá»‡m & so sÃ¡nh

- TrÃªn bá»™ dá»¯ liá»‡u PhysioNet, phÆ°Æ¡ng phÃ¡p GA-Shapley chá»n 20 features vÃ  Ä‘áº¡t AUC â‰ˆ 0.80â€“0.81  
- VÆ°á»£t háº§u háº¿t phÆ°Æ¡ng phÃ¡p truyá»n thá»‘ng (Ï‡Â², tree-based, Relief) trong má»¥c tiÃªu tÄƒng sensitivity cho bÃ i toÃ¡n false alarm reduction.

**LÆ°u Ã½:**  
PhÆ°Æ¡ng phÃ¡p nÃ y Æ°á»›c lÆ°á»£ng Shapley (approximate) â€” khÃ´ng cho káº¿t quáº£ chÃ­nh xÃ¡c tuyá»‡t Ä‘á»‘i nhÆ°ng Ä‘Ã¡nh Ä‘á»•i báº±ng tiáº¿t kiá»‡m tÃ­nh toÃ¡n vÃ  kháº£ nÄƒng má»Ÿ rá»™ng tá»›i táº­p feature lá»›n (hÃ ng trÄƒm).

---

**Nguá»“n tham kháº£o:**  
Zaeri-Amirani et al., 2018.
CÃ¡c hÆ°á»›ng giáº£i thÃ­ch giÃ¡n tiáº¿p cho GA (Indirect Explainability for Genetic Algorithms)## 7. HÆ°á»›ng phÃ¡t triá»ƒn: Explainable Evolutionary Computation (EEC)

TÆ°Æ¡ng lai cÃ³ thá»ƒ hÃ¬nh thÃ nh má»™t nhÃ¡nh nghiÃªn cá»©u má»›i mang tÃªn **Explainable Evolutionary Computation (EEC)** â€“ hÆ°á»›ng tá»›i viá»‡c **tÃ­ch há»£p kháº£ nÄƒng tá»± giáº£i thÃ­ch (self-explainability)** vÃ o chÃ­nh cÃ¡c thuáº­t toÃ¡n tiáº¿n hoÃ¡.

Ba hÆ°á»›ng phÃ¡t triá»ƒn tiá»m nÄƒng gá»“m:

1. **Surrogate explainability:**  
   GA Ä‘Æ°á»£c mÃ´ phá»ng (surrogate) bá»Ÿi má»™t mÃ´ hÃ¬nh há»c mÃ¡y, sau Ä‘Ã³ mÃ´ hÃ¬nh nÃ y Ä‘Æ°á»£c giáº£i thÃ­ch báº±ng SHAP.  
   â†’ CÃ¡ch tiáº¿p cáº­n giÃ¡n tiáº¿p, cho phÃ©p hiá»ƒu Ä‘Æ°á»£c má»‘i quan há»‡ giá»¯a cáº¥u trÃºc gene vÃ  fitness.

2. **In-process explainability:**  
   GA Ä‘Æ°á»£c má»Ÿ rá»™ng Ä‘á»ƒ tá»± ghi nháº­n cÃ¡c thá»‘ng kÃª trong quÃ¡ trÃ¬nh tiáº¿n hoÃ¡ â€” cháº³ng háº¡n nhÆ° **gene importance**, **mutation impact**, hoáº·c **selection pressure** theo thá»i gian.  
   â†’ CÃ¡ch nÃ y biáº¿n GA thÃ nh há»‡ thá»‘ng cÃ³ â€œnháº­t kÃ½ tiáº¿n hoÃ¡â€ cÃ³ thá»ƒ phÃ¢n tÃ­ch trá»±c tiáº¿p.

3. **Dual explainability:**  
   GA tÃ­ch há»£p má»™t **Shapley-like metric ná»™i táº¡i** Ä‘á»ƒ Ä‘o lÆ°á»ng Ä‘Ã³ng gÃ³p cá»§a tá»«ng gene ngay trong quÃ¡ trÃ¬nh tiáº¿n hoÃ¡.  
   â†’ Khi Ä‘Ã³, â€œSHAP cho GAâ€ xuáº¥t hiá»‡n á»Ÿ táº§ng **meta**, tá»©c lÃ  GA **tá»± Ä‘Ã¡nh giÃ¡ táº§m quan trá»ng gene cá»§a mÃ¬nh** mÃ  khÃ´ng cáº§n mÃ´ hÃ¬nh ngoÃ i.

Nhá»¯ng hÆ°á»›ng nÃ y má»Ÿ ra kháº£ nÄƒng phÃ¡t triá»ƒn cÃ¡c há»‡ tiáº¿n hoÃ¡ **tá»± quan sÃ¡t, tá»± Ä‘Ã¡nh giÃ¡, vÃ  tá»± Ä‘iá»u chá»‰nh**, gÃ³p pháº§n Ä‘á»‹nh hÃ¬nh má»™t tháº¿ há»‡ **thuáº­t toÃ¡n tiáº¿n hoÃ¡ cÃ³ thá»ƒ giáº£i thÃ­ch Ä‘Æ°á»£c** (explainable evolutionary algorithms).

---

## 4.6. Káº¿t luáº­n

SHAP lÃ  cÃ´ng cá»¥ máº¡nh máº½ cho **model explainability**, nhÆ°ng chá»‰ khi **Ä‘á»‘i tÆ°á»£ng lÃ  mÃ´ hÃ¬nh cÃ³ hÃ m Ä‘áº§u ra xÃ¡c Ä‘á»‹nh**.  
NgÆ°á»£c láº¡i, **Genetic Algorithm** lÃ  má»™t **quÃ¡ trÃ¬nh tÃ¬m kiáº¿m tiáº¿n hoÃ¡ ngáº«u nhiÃªn**, khÃ´ng cÃ³ cáº¥u trÃºc hÃ m cá»‘ Ä‘á»‹nh.

VÃ¬ váº­y:

- SHAP **khÃ´ng thá»ƒ trá»±c tiáº¿p giáº£i thÃ­ch** GA,  
- nhÆ°ng **cÃ³ thá»ƒ giáº£i thÃ­ch giÃ¡n tiáº¿p** hÃ nh vi hoáº·c káº¿t quáº£ mÃ  GA táº¡o ra,  
- vÃ  **GA láº¡i cÃ³ thá»ƒ há»— trá»£ SHAP** trong viá»‡c giáº£m chi phÃ­ tÃ­nh toÃ¡n giÃ¡ trá»‹ Shapley.

> SHAP giÃºp ta hiá»ƒu mÃ´ hÃ¬nh;  
> GA giÃºp ta tÃ¬m lá»i giáº£i;  
> Káº¿t há»£p cáº£ hai â€“ ta tiáº¿n gáº§n hÆ¡n tá»›i má»™t **trÃ­ tuá»‡ nhÃ¢n táº¡o vá»«a máº¡nh, vá»«a minh báº¡ch.**



# 5. Case Study: Tá»‘i Æ°u Ä‘Æ°á»ng Ä‘i cá»§a robot giao hÃ ng (Delivery Route Optimization)

Sau khi Ä‘Ã£ hiá»ƒu rÃµ **Genetic Algorithm (GA)** lÃ  gÃ¬ vÃ  **SHAP** cÃ³ thá»ƒ giÃºp ta nhÃ¬n tháº¥u â€œhá»™p Ä‘enâ€ cá»§a mÃ´ hÃ¬nh ra sao, hÃ£y cÃ¹ng chuyá»ƒn sang má»™t vÃ­ dá»¥ thá»±c tiá»…n cá»¥ thá»ƒ â€” nÆ¡i **GA thá»±c sá»± phÃ¡t huy sá»©c máº¡nh tá»‘i Æ°u hÃ³a**.


## 5.1. Tá»« bá»‘i cáº£nh Ä‘áº¿n mÃ´ hÃ¬nh bÃ i toÃ¡n

Trong cÃ¡c thÃ nh phá»‘ hiá»‡n Ä‘áº¡i, dá»‹ch vá»¥ giao hÃ ng tá»± Ä‘á»™ng báº±ng **robot di chuyá»ƒn máº·t Ä‘áº¥t (AGV â€“ Autonomous Ground Vehicle)** Ä‘ang ngÃ y cÃ ng phá»• biáº¿n.  
Má»™t cÃ´ng ty logistic cáº§n Ä‘iá»u phá»‘i nhiá»u robot giao hÃ ng trong khu cÃ´ng nghiá»‡p, sao cho **má»—i robot hoÃ n thÃ nh táº¥t cáº£ Ä‘iá»ƒm giao vá»›i quÃ£ng Ä‘Æ°á»ng ngáº¯n nháº¥t**, **tiáº¿t kiá»‡m nÄƒng lÆ°á»£ng**, vÃ  **khÃ´ng trá»… thá»i gian giao hÃ ng**.

Náº¿u mÃ´ hÃ¬nh hÃ³a bÃ i toÃ¡n nÃ y, ta tháº¥y nÃ³ lÃ  biáº¿n thá»ƒ cá»§a **Traveling Salesman Problem (TSP)** â€” má»™t trong nhá»¯ng bÃ i toÃ¡n **NP-hard** kinh Ä‘iá»ƒn trong tá»‘i Æ°u tá»• há»£p.

Má»¥c tiÃªu cá»§a ta lÃ  tÃ¬m **chu trÃ¬nh tá»‘i Æ°u** Ä‘i qua táº¥t cáº£ cÃ¡c Ä‘iá»ƒm giao hÃ ng vÃ  quay vá» kho, vá»›i chi phÃ­ tá»•ng thá»ƒ tá»‘i thiá»ƒu.

---

### 5.1.1. MÃ´ hÃ¬nh hÃ³a toÃ¡n há»c

Giáº£ sá»­ cÃ³:
- $N$ Ä‘iá»ƒm giao hÃ ng (khÃ´ng tÃ­nh kho)
- Kho hÃ ng kÃ½ hiá»‡u lÃ  Ä‘iá»ƒm $0$
- Ma tráº­n khoáº£ng cÃ¡ch $D = [d_{ij}]$, trong Ä‘Ã³ $d_{ij}$ lÃ  quÃ£ng Ä‘Æ°á»ng giá»¯a Ä‘iá»ƒm $i$ vÃ  $j$

Khi Ä‘Ã³, Ä‘Æ°á»ng Ä‘i (hay â€œchromosomeâ€) Ä‘Æ°á»£c biá»ƒu diá»…n báº±ng má»™t hoÃ¡n vá»‹:
$$
R = [0, r_1, r_2, \dots, r_N, 0]
$$

Tá»•ng quÃ£ng Ä‘Æ°á»ng pháº£i di chuyá»ƒn lÃ :
$$
L(R) = \sum_{i=0}^{N} d_{r_i, r_{i+1}}
$$

---

### 5.1.2. HÃ m má»¥c tiÃªu (Fitness Function)

Äá»ƒ mÃ´ hÃ¬nh pháº£n Ã¡nh thá»±c táº¿, ngoÃ i khoáº£ng cÃ¡ch, ta cáº§n tÃ­nh Ä‘áº¿n:
- NÄƒng lÆ°á»£ng tiÃªu hao $E(R)$ (tÄƒng theo táº£i trá»ng vÃ  quÃ£ng Ä‘Æ°á»ng)
- Pháº¡t trá»… thá»i gian $P(R)$ náº¿u giao hÃ ng muá»™n hÆ¡n deadline

HÃ m tá»‘i Æ°u tá»•ng quÃ¡t lÃ :

$$
\text{Minimize } F(R) = \alpha \times L(R) + \beta \times E(R) + \gamma \times P(R)
$$

vá»›i $\alpha, \beta, \gamma$ lÃ  cÃ¡c há»‡ sá»‘ trá»ng sá»‘ (trade-off giá»¯a Ä‘á»™ dÃ i, nÄƒng lÆ°á»£ng vÃ  Ä‘á»™ trá»…).  
Trong GA, ta thÆ°á»ng Ä‘á»‹nh nghÄ©a **hÃ m fitness** tá»· lá»‡ nghá»‹ch vá»›i chi phÃ­ nÃ y:

$$
\text{Fitness}(R) = \frac{1}{F(R) + \varepsilon}
$$

vá»›i $\varepsilon$ lÃ  háº±ng sá»‘ ráº¥t nhá» Ä‘á»ƒ trÃ¡nh chia cho 0.


## 5.2. CÃ¡ch GA hoáº¡t Ä‘á»™ng trong bÃ i toÃ¡n nÃ y

HÃ£y xem cÃ¡ch GA â€œtiáº¿n hÃ³aâ€ Ä‘á»ƒ tÃ¬m lá»i giáº£i tá»‘i Æ°u cho bÃ i toÃ¡n trÃªn.

### **BÆ°á»›c 1 â€“ Khá»Ÿi táº¡o quáº§n thá»ƒ**

Táº¡o ngáº«u nhiÃªn $M$ tuyáº¿n Ä‘Æ°á»ng khÃ¡c nhau, má»—i tuyáº¿n lÃ  má»™t hoÃ¡n vá»‹ há»£p lá»‡ cá»§a cÃ¡c Ä‘iá»ƒm giao hÃ ng.

VÃ­ dá»¥:
```
CÃ¡ thá»ƒ 1: [0, 2, 4, 1, 3, 0]
CÃ¡ thá»ƒ 2: [0, 3, 1, 4, 2, 0]
...
```

Má»—i cÃ¡ thá»ƒ biá»ƒu diá»…n **má»™t phÆ°Æ¡ng Ã¡n lá»™ trÃ¬nh** mÃ  robot cÃ³ thá»ƒ Ä‘i.


### **BÆ°á»›c 2 â€“ ÄÃ¡nh giÃ¡ fitness**

TÃ­nh **fitness** cho tá»«ng cÃ¡ thá»ƒ dá»±a trÃªn hÃ m $F(R)$ Ä‘Ã£ nÃªu:

$$
\text{Fitness}(R) = \frac{1}{\alpha L(R) + \beta E(R) + \gamma P(R)}
$$

â†’ CÃ¡ thá»ƒ cÃ³ **fitness cao** nghÄ©a lÃ  tuyáº¿n Ä‘Æ°á»ng **ngáº¯n**, **tiáº¿t kiá»‡m nÄƒng lÆ°á»£ng**, vÃ  **Ã­t trá»… háº¹n**.


### **BÆ°á»›c 3 â€“ Chá»n lá»c (Selection)**

Chá»n cÃ¡c cÃ¡ thá»ƒ tá»‘t Ä‘á»ƒ lÃ m â€œbá»‘ máº¹â€.  
Hai cÃ¡ch chá»n phá»• biáº¿n:

- **Roulette Wheel Selection:**  
  XÃ¡c suáº¥t chá»n tá»· lá»‡ vá»›i fitness. CÃ¡ thá»ƒ tá»‘t cÃ³ nhiá»u â€œpháº§n bÃ¡nhâ€ hÆ¡n.
- **Tournament Selection:**  
  Chá»n ngáº«u nhiÃªn $k$ cÃ¡ thá»ƒ, láº¥y cÃ¡ thá»ƒ cÃ³ fitness cao nháº¥t.


### **BÆ°á»›c 4 â€“ Lai ghÃ©p (Crossover)**

Hai cÃ¡ thá»ƒ bá»‘ máº¹ káº¿t há»£p gen Ä‘á»ƒ sinh ra cÃ¡ thá»ƒ con.  
Vá»›i bÃ i toÃ¡n TSP, ta dÃ¹ng **Order Crossover (OX)**:

| Cha | 0  | 2 | 4 | 1 | 3 | 0 |
|-----|----|---|---|---|---|---|
| Máº¹  | 0  | 3 | 1 | 4 | 2 | 0 |

- Giá»¯ nguyÃªn Ä‘oáº¡n tá»« cha, Ä‘iá»n pháº§n cÃ²n láº¡i theo thá»© tá»± máº¹ â†’ Ä‘áº£m báº£o khÃ´ng trÃ¹ng láº·p Ä‘iá»ƒm.

---

### **BÆ°á»›c 5 â€“ Äá»™t biáº¿n (Mutation)**

HoÃ¡n Ä‘á»•i ngáº«u nhiÃªn 2 Ä‘iá»ƒm trong lá»™ trÃ¬nh Ä‘á»ƒ duy trÃ¬ Ä‘a dáº¡ng quáº§n thá»ƒ:

$$
R' = [0, 3, 1, 2, 4, 0] \rightarrow [0, 3, \underline{4}, 2, \underline{1}, 0]
$$

â†’ Äá»™t biáº¿n giÃºp **trÃ¡nh káº¹t á»Ÿ cá»±c trá»‹ cá»¥c bá»™ (local optima)**.


### **BÆ°á»›c 6 â€“ Táº¡o tháº¿ há»‡ má»›i**

- Giá»¯ láº¡i má»™t sá»‘ cÃ¡ thá»ƒ tá»‘t nháº¥t (elitism).  
- ThÃªm cÃ¡c cÃ¡ thá»ƒ con má»›i Ä‘Æ°á»£c lai/Ä‘á»™t biáº¿n.

Láº·p láº¡i quÃ¡ trÃ¬nh nÃ y qua **nhiá»u tháº¿ há»‡ (generations)** cho Ä‘áº¿n khi fitness há»™i tá»¥.


## 5.3. Minh há»a Genetic Algorithm giÃºp cho Robot giao hÃ ng
- Minh hoáº¡ thá»ƒ hiá»‡n cÃ¡ch GA tiáº¿n hoÃ¡ Ä‘á»ƒ tá»‘i Æ°u hoÃ¡ lá»™ trÃ¬nh giao hÃ ng cho 1 robot vá»›i 15 Ä‘iá»ƒm giao:

### Tuyáº¿n Ä‘Æ°á»ng tá»‘i Æ°u:
![Tuyáº¿n Ä‘Æ°á»ng tá»‘i Æ°u cá»§a robot giao hÃ ng](images/5.GA-run.png)

### Tiáº¿n trÃ¬nh tá»‘i Æ°u (Fitness qua cÃ¡c tháº¿ há»‡)
![Tiáº¿n trÃ¬nh tá»‘i Æ°u](images/5.GA-vis.png)

## PhÃ¢n tÃ­ch káº¿t quáº£

Sau vÃ i trÄƒm tháº¿ há»‡, GA dáº§n tÃ¬m ra tuyáº¿n Ä‘Æ°á»ng cÃ³:
- Tá»•ng quÃ£ng Ä‘Æ°á»ng ngáº¯n hÆ¡n ~10â€“15% so vá»›i random baseline  
- NÄƒng lÆ°á»£ng tiÃªu hao tháº¥p hÆ¡n (vÃ¬ Ä‘Æ°á»ng Ä‘i há»£p lÃ½ hÆ¡n)  
- KhÃ´ng trá»… deadline (náº¿u cÃ³ rÃ ng buá»™c thá»i gian)

QuÃ¡ trÃ¬nh tiáº¿n hÃ³a thá»ƒ hiá»‡n **tinh tháº§n Darwin** rÃµ rá»‡t:
- Tháº¿ há»‡ Ä‘áº§u â†’ há»—n loáº¡n, lá»™ trÃ¬nh dÃ i vÃ  tá»‘n nÄƒng lÆ°á»£ng  
- Tháº¿ há»‡ giá»¯a â†’ cÃ¡c gen â€œtá»‘tâ€ (cá»¥m Ä‘iá»ƒm gáº§n nhau) dáº§n Ä‘Æ°á»£c giá»¯ láº¡i  
- Tháº¿ há»‡ cuá»‘i â†’ lá»™ trÃ¬nh há»™i tá»¥, fitness gáº§n nhÆ° bÃ£o hÃ²a

---

## 5.4. Ã nghÄ©a vÃ  má»Ÿ rá»™ng

### (1) Vá» máº·t ká»¹ thuáº­t
GA chá»©ng minh kháº£ nÄƒng giáº£i bÃ i toÃ¡n **tá»‘i Æ°u phi tuyáº¿n, khÃ´ng kháº£ vi** â€” nÆ¡i Gradient Descent báº¥t lá»±c.  
BÃ i toÃ¡n Delivery Route Optimization lÃ  vÃ­ dá»¥ kinh Ä‘iá»ƒn thá»ƒ hiá»‡n:
- **TÃ­nh toÃ n cá»¥c (global search)**  
- **Kháº£ nÄƒng thoÃ¡t local optima**  
- **Kháº£ nÄƒng xá»­ lÃ½ rÃ ng buá»™c Ä‘a má»¥c tiÃªu**

### (2) Vá» máº·t nghiÃªn cá»©u
Khi káº¿t há»£p vá»›i cÃ¡c mÃ´ hÃ¬nh **Explainability (vÃ­ dá»¥ SHAP)** nhÆ° pháº§n 6 sáº¯p tá»›i, ta cÃ³ thá»ƒ:
- PhÃ¢n tÃ­ch **gene nÃ o** trong lá»™ trÃ¬nh (vÃ­ dá»¥: nhÃ³m Ä‘iá»ƒm gáº§n nhau, hoáº·c gÃ³c ráº½ cá»¥ thá»ƒ) áº£nh hÆ°á»Ÿng máº¡nh nháº¥t Ä‘áº¿n fitness  
- Giáº£i thÃ­ch táº¡i sao GA há»™i tá»¥ theo hÆ°á»›ng Ä‘Ã³  
- Thiáº¿t káº¿ láº¡i tham sá»‘ mutation/crossover cÃ³ cÆ¡ sá»Ÿ hÆ¡n

---

> Tá»« má»™t bÃ i toÃ¡n giao hÃ ng tÆ°á»Ÿng chá»«ng Ä‘Æ¡n giáº£n, ta Ä‘Ã£ tháº¥y **Genetic Algorithm** mÃ´ phá»ng trá»n váº¹n quy luáº­t tiáº¿n hÃ³a tá»± nhiÃªn:  
> chá»n lá»c, lai ghÃ©p, Ä‘á»™t biáº¿n, vÃ  tiáº¿n hÃ³a dáº§n Ä‘áº¿n tá»‘i Æ°u.  
> ÄÃ¢y cÅ©ng chÃ­nh lÃ  sá»©c máº¡nh khiáº¿n GA trá»Ÿ thÃ nh cÃ´ng cá»¥ ná»n táº£ng cho nhiá»u há»‡ thá»‘ng **AI mÃ´ phá»ng tá»± nhiÃªn (Nature-inspired AI)** hiá»‡n nay.

# 6. Äá» xuáº¥t cáº£i tiáº¿n
á» pháº§n nÃ y, ta sáº½ Ä‘i sÃ¢u vÃ o cÃ¡c phÆ°Æ¡ng phÃ¡p cáº£i tiáº¿n ná»•i báº­t dá»±a trÃªn lÃ½ thuyáº¿t SHAP Ä‘á»ƒ giáº£i thÃ­ch mÃ´ hÃ¬nh há»c mÃ¡y, bao gá»“m  **Tree SHAP**,  **Kernel SHAP**  vÃ   **Deep SHAP**. Má»—i phÆ°Æ¡ng phÃ¡p cÃ³ nhá»¯ng Ä‘iá»ƒm máº¡nh riÃªng Ä‘á»ƒ giáº£i quyáº¿t cÃ¡c háº¡n cháº¿ trong viá»‡c tÃ­nh toÃ¡n giÃ¡ trá»‹ Shapley truyá»n thá»‘ng.

## 6.1. Tree SHAP - tÄƒng tá»‘c cho mÃ´ hÃ¬nh cÃ¢y
Tree SHAP lÃ  thuáº­t toÃ¡n tá»‘i Æ°u Ä‘á»ƒ tÃ­nh giÃ¡ trá»‹ SHAP (Shapley Additive Explanations) cho cÃ¡c mÃ´ hÃ¬nh dá»±a trÃªn cÃ¢y quyáº¿t Ä‘á»‹nh nhÆ° XGBoost, LightGBM, CatBoost... NÃ³ dá»±a trÃªn lÃ½ thuyáº¿t giÃ¡ trá»‹ Shapley trong trÃ² chÆ¡i há»£p tÃ¡c, chia Ä‘á»u káº¿t quáº£ dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh cho tá»«ng Ä‘áº·c trÆ°ng má»™t cÃ¡ch cÃ´ng báº±ng.
### NguyÃªn lÃ½ tÃ­nh toÃ¡n Tree SHAP

-   GiÃ¡ trá»‹ SHAP truyá»n thá»‘ng yÃªu cáº§u tÃ­nh trung bÃ¬nh Ä‘Ã¡nh giÃ¡ sá»± Ä‘Ã³ng gÃ³p cá»§a Ä‘áº·c trÆ°ng  $i$  trÃªn  má»i táº­p con cÃ³ hoáº·c khÃ´ng cÃ³ Ä‘áº·c trÆ°ng Ä‘Ã³, dáº«n Ä‘áº¿n Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n theo hÃ m mÅ©  $O(2^{|N|})$, vá»›i  $âˆ£Nâˆ£$  lÃ  sá»‘ Ä‘áº·c trÆ°ng.
    
-   Tree SHAP sá»­ dá»¥ng Ä‘áº·c thÃ¹ cáº¥u trÃºc cÃ¢y Ä‘á»ƒ tÃ­nh toÃ¡n giÃ¡ trá»‹ nÃ y  má»™t cÃ¡ch hiá»‡u quáº£ báº±ng thuáº­t toÃ¡n Ä‘á»‡ quy duyá»‡t cÃ¡c nhÃ¡nh cÃ¢y, tÃ­nh trá»ng sá»‘ xÃ¡c suáº¥t máº«u Ä‘i qua tá»«ng nhÃ¡nh dá»±a trÃªn táº­p Ä‘áº·c trÆ°ng tham gia, mÃ  khÃ´ng cáº§n xÃ©t háº¿t táº¥t cáº£ táº­p con.
    
-   Thuáº­t toÃ¡n táº­n dá»¥ng tÃ­nh cháº¥t  tÃ­nh cá»™ng cá»§a Shapley  (SHAP cho mÃ´ hÃ¬nh rá»«ng cÃ¢y lÃ  tá»•ng SHAP cá»§a tá»«ng cÃ¢y) vÃ  cáº¥u trÃºc phÃ¢n nhÃ¡nh cá»§a cÃ¢y, giáº£m Ä‘á»™ phá»©c táº¡p xuá»‘ng Ä‘a thá»©c:  $O(TLD^2)$, vá»›i  $T$  sá»‘ cÃ¢y,  $L$  sá»‘ lÃ¡ tá»‘i Ä‘a má»—i cÃ¢y,  $D$  Ä‘á»™ sÃ¢u tá»‘i Ä‘a cÃ¢y.

DÆ°á»›i Ä‘Ã¢y lÃ  báº£ng so sÃ¡nh sá»± khÃ¡c biá»‡t giá»¯a SHAP vÃ  Tree SHAP:
| TiÃªu chÃ­| SHAP ThÆ°á»ng| Tree SHAP  
|-|-|-|
| Pháº¡m vi Ã¡p dá»¥ng | Má»i mÃ´ hÃ¬nh (model-agnostic) | MÃ´ hÃ¬nh cÃ¢y vÃ  ensemble cÃ¢y (XGBoost, LightGBM, Random Forest)|
| CÃ´ng thá»©c cÆ¡ báº£n | GiÃ¡ trá»‹ Shapley trung bÃ¬nh trÃªn má»i táº­p con| Thuáº­t toÃ¡n Ä‘á»‡ quy trÃªn cÃ¢y tÃ­nh trá»ng sá»‘ xÃ¡c suáº¥t tá»«ng nhÃ¡nh|
| Äá»™ phá»©c táº¡p tÃ­nh toÃ¡n | $O(2^{\|N\|})$, vá»›i  $âˆ£Nâˆ£$  lÃ  sá»‘ Ä‘áº·c trÆ°ng|$O(TLD^2)$, vá»›i  $T$  sá»‘ cÃ¢y,  $L$  sá»‘ lÃ¡ tá»‘i Ä‘a má»—i cÃ¢y,  $D$  Ä‘á»™ sÃ¢u tá»‘i Ä‘a cÃ¢y |
| Tá»‘c Ä‘á»™| Ráº¥t cháº­m vá»›i Ä‘áº·c trÆ°ng nhiá»u| Nhanh, sá»­ dá»¥ng cáº¥u trÃºc cÃ¢y tá»‘i Æ°u hÃ³a|



## 6.2. Kernel SHAP â€“ linh hoáº¡t cho mÃ´ hÃ¬nh phi tuyáº¿n
Kernel SHAP lÃ  má»™t phÆ°Æ¡ng phÃ¡p **model-agnostic** giÃºp tÃ­nh toÃ¡n giÃ¡ trá»‹ SHAP â€“ giÃ¡ trá»‹ Ä‘Ã³ng gÃ³p cÃ´ng báº±ng cá»§a tá»«ng Ä‘áº·c trÆ°ng â€“ cho má»i mÃ´ hÃ¬nh há»c mÃ¡y, ká»ƒ cáº£ nhá»¯ng mÃ´ hÃ¬nh phi tuyáº¿n vÃ  há»™p Ä‘en (black-box) mÃ  ta khÃ´ng hiá»ƒu rÃµ cáº¥u trÃºc bÃªn trong.
### NguyÃªn lÃ½ hoáº¡t Ä‘á»™ng

-   Kernel SHAP biáº¿n bÃ i toÃ¡n tÃ­nh giÃ¡ trá»‹ Shapley thÃ nh má»™t bÃ i toÃ¡n há»“i quy tuyáº¿n tÃ­nh trá»ng sá»‘ cá»¥c bá»™. Thuáº­t toÃ¡n láº¥y máº«u cÃ¡c táº­p con Ä‘áº·c trÆ°ng (hoáº·c "liÃªn minh"), vá»›i má»—i táº­p con biá»ƒu diá»…n má»™t cÃ¡ch mÃ´ phá»ng viá»‡c biáº¿n Ä‘áº·c trÆ°ng "cÃ³ máº·t" hay "bá»‹ áº©n" khi dá»± Ä‘oÃ¡n.
 -   Äá»ƒ áº©n Ä‘áº·c trÆ°ng, Kernel SHAP thay giÃ¡ trá»‹ Ä‘áº·c trÆ°ng Ä‘Ã³ báº±ng giÃ¡ trá»‹ trung bÃ¬nh hoáº·c máº«u tá»« má»™t bá»™ dá»¯ liá»‡u ná»n (background dataset), Ä‘áº£m báº£o giáº£ Ä‘á»‹nh cÃ¡c Ä‘áº·c trÆ°ng Ä‘á»™c láº­p.
-   Thuáº­t toÃ¡n Ä‘Ã¡nh trá»ng sá»‘ cÃ¡c táº­p con dá»±a trÃªn kÃ­ch thÆ°á»›c táº­p con theo hÃ m kernel Ä‘áº·c biá»‡t nháº±m giá»¯ tÃ­nh cÃ´ng báº±ng cá»§a giÃ¡ trá»‹ Shapley.
-   Sau khi láº¥y máº«u, Kernel SHAP thá»±c hiá»‡n há»“i quy tuyáº¿n tÃ­nh trá»ng sá»‘ giá»¯a giÃ¡ trá»‹ dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh trÃªn cÃ¡c táº­p con vÃ  cÃ¡c Ä‘áº·c trÆ°ng cÃ³ máº·t, tá»« Ä‘Ã³ Æ°á»›c lÆ°á»£ng trá»ng sá»‘ Ä‘Ã³ng gÃ³p (SHAP value) cá»§a tá»«ng Ä‘áº·c trÆ°ng.

### CÃ´ng thá»©c trá»ng sá»‘ Kernel SHAP

HÃ m trá»ng sá»‘ kernel cho má»™t táº­p con Ä‘áº·c trÆ°ng  $S$, táº­p Ä‘áº·c trÆ°ng Ä‘áº§y Ä‘á»§  $N$  vá»›i  $M=âˆ£Nâˆ£$, Ä‘Æ°á»£c tÃ­nh báº±ng:
$$
\omega(S) = \frac{M - 1}{\binom{M}{|S|} \times |S| \times (M - |S|)}
$$
CÃ´ng thá»©c nÃ y cÃ¢n báº±ng trá»ng sá»‘ Ä‘á»ƒ Ä‘áº£m báº£o má»—i Ä‘áº·c trÆ°ng Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡ Ä‘Ãºng má»©c qua máº«u táº­p con.

### Sá»± khÃ¡c biá»‡t giá»¯a SHAP vÃ  Kernel SHAP
| TiÃªu chÃ­             | SHAP (Permutation SHAP)| Kernel SHAP|
|----------------------|-------------------------------------|------------|
| Äá»™ phá»©c táº¡p tÃ­nh toÃ¡n | TÆ°Æ¡ng Ä‘Æ°Æ¡ng, ráº¥t lá»›n vÃ  Ä‘á»™t biáº¿n theo sá»‘ Ä‘áº·c trÆ°ng (hÃ m mÅ©) | Xáº¥p xá»‰, tÃ­nh gáº§n Ä‘Ãºng vá»›i hiá»‡u quáº£ cao, Ä‘áº·c biá»‡t khi nhiá»u Ä‘áº·c trÆ°ng |
| Giáº£ Ä‘á»‹nh tÃ­nh toÃ¡n    | TÃ­nh toÃ¡n chÃ­nh xÃ¡c má»‘i liÃªn há»‡ Ä‘áº·c trÆ°ng, khÃ´ng giáº£ Ä‘á»‹nh Ä‘á»™c láº­p | Giáº£ Ä‘á»‹nh Ä‘áº·c trÆ°ng Ä‘á»™c láº­p, cÃ³ thá»ƒ sai lá»‡ch vá»›i dá»¯ liá»‡u cÃ³ tÆ°Æ¡ng quan |
| Äá»™ chÃ­nh xÃ¡c| ChÃ­nh xÃ¡c nháº¥t | Gáº§n báº±ng chÃ­nh xÃ¡c, Ä‘Ã´i khi khÃ¡c biá»‡t vá»›i dá»¯ liá»‡u cÃ³ tÆ°Æ¡ng quan Ä‘áº·c trÆ°ng |
| á»¨ng dá»¥ng| DÃ¹ng khi sá»‘ Ä‘áº·c trÆ°ng nhá», Æ°u tiÃªn Ä‘á»™ chÃ­nh xÃ¡c     | Phá»• biáº¿n vá»›i sá»‘ Ä‘áº·c trÆ°ng lá»›n vÃ  mÃ´ hÃ¬nh há»™p Ä‘en khÃ´ng biáº¿t cáº¥u trÃºc |


## 6.3. Deep SHAP â€“ giáº£i thÃ­ch cho máº¡ng nÆ¡-ron sÃ¢u

Deep SHAP lÃ  má»™t phÆ°Æ¡ng phÃ¡p giáº£i thÃ­ch dá»±a trÃªn khung SHAP Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘áº·c biá»‡t cho cÃ¡c mÃ´ hÃ¬nh  **máº¡ng nÆ¡-ron sÃ¢u**  (Deep Neural Networks - DNNs). NÃ³ káº¿t há»£p lÃ½ thuyáº¿t giÃ¡ trá»‹ Shapley vá»›i cÃ¡c ká»¹ thuáº­t lan truyá»n ngÆ°á»£c (backpropagation) trong máº¡ng tháº§n kinh Ä‘á»ƒ Æ°á»›c tÃ­nh vÃ  phÃ¢n phá»‘i cÃ´ng báº±ng áº£nh hÆ°á»Ÿng cá»§a tá»«ng Ä‘áº·c trÆ°ng Ä‘áº§u vÃ o lÃªn káº¿t quáº£ dá»± Ä‘oÃ¡n.

### LÃ½ do ra Ä‘á»i cá»§a Deep SHAP

-   Máº¡ng nÆ¡-ron sÃ¢u vá»›i hÃ ng chá»¥c Ä‘áº¿n hÃ ng trÄƒm lá»›p áº©n cÃ³ mÃ´ hÃ¬nh ráº¥t phá»©c táº¡p, thÆ°á»ng Ä‘Æ°á»£c xem nhÆ° "há»™p Ä‘en" do khÃ³ Ä‘oÃ¡n sá»± Ä‘Ã³ng gÃ³p cá»§a tá»«ng Ä‘áº§u vÃ o.
    
-   CÃ¡c phÆ°Æ¡ng phÃ¡p SHAP truyá»n thá»‘ng (nhÆ° Kernel SHAP) cÃ³ thá»ƒ Ã¡p dá»¥ng nhÆ°ng thÆ°á»ng ráº¥t tá»‘n kÃ©m vá» tÃ­nh toÃ¡n vÃ  khÃ´ng táº­n dá»¥ng Ä‘Æ°á»£c cáº¥u trÃºc Ä‘áº·c thÃ¹ cá»§a máº¡ng nÆ¡-ron sÃ¢u.
    
-   Deep SHAP táº­n dá»¥ng Ä‘áº·c Ä‘iá»ƒm cáº¥u trÃºc máº¡ng nÆ¡-ron Ä‘á»ƒ giáº£i thÃ­ch mÃ´ hÃ¬nh má»™t cÃ¡ch nhanh hÆ¡n vÃ  chÃ­nh xÃ¡c hÆ¡n.
    

### NguyÃªn lÃ½ hoáº¡t Ä‘á»™ng

-   Deep SHAP dá»±a trÃªn sá»± káº¿t há»£p cá»§a SHAP vÃ   **DeepLIFT**  â€“ má»™t phÆ°Æ¡ng phÃ¡p phÃ¢n tÃ­ch Ä‘á»™ Ä‘Ã³ng gÃ³p dá»± Ä‘oÃ¡n thÃ´ng qua ká»¹ thuáº­t lan truyá»n ngÆ°á»£c (backpropagation) trong máº¡ng neural.
    
-   DeepLIFT so sÃ¡nh giÃ¡ trá»‹ cá»§a cÃ¡c nÆ¡-ron so vá»›i má»™t giÃ¡ trá»‹ tham chiáº¿u (baseline) vÃ  tÃ­nh toÃ¡n sá»± thay Ä‘á»•i nÃ y truyá»n ngÆ°á»£c vá» Ä‘áº§u vÃ o, thá»ƒ hiá»‡n Ä‘Ã³ng gÃ³p cá»§a tá»«ng Ä‘áº·c trÆ°ng.
    
-   Deep SHAP sá»­ dá»¥ng cÃ¡c giÃ¡ trá»‹ nÃ y Ä‘á»ƒ xáº¥p xá»‰  giÃ¡ trá»‹ Shapley, Ä‘á»“ng thá»i Ä‘áº£m báº£o tÃ­nh cháº¥t cÃ´ng báº±ng vÃ  cÃ¡c Ä‘áº·c Ä‘iá»ƒm toÃ¡n há»c cá»§a giÃ¡ trá»‹ SHAP.

### Æ¯u Ä‘iá»ƒm cá»§a Deep SHAP

-   **Hiá»‡u quáº£ tÃ­nh toÃ¡n:**  táº­n dá»¥ng viá»‡c lan truyá»n ngÆ°á»£c nÃªn nhanh hÆ¡n nhiá»u so vá»›i phÆ°Æ¡ng phÃ¡p mÃ´ phá»ng táº­p con hay Kernel SHAP.
    
-   **ChÃ­nh xÃ¡c hÆ¡n cÃ¡c phÆ°Æ¡ng phÃ¡p gradient thuáº§n tÃºy:**  vÃ¬ Ä‘Ã¡nh giÃ¡ dá»±a trÃªn thay Ä‘á»•i tÆ°Æ¡ng Ä‘á»‘i so vá»›i baseline, giáº£m sai lá»‡ch do phi tuyáº¿n.
    
-   **Giá»¯ Ä‘Æ°á»£c tÃ­nh cÃ´ng báº±ng cá»§a Shapley values:**  Ä‘áº£m báº£o má»—i Ä‘áº·c trÆ°ng nháº­n Ä‘Ã³ng gÃ³p phÃ¹ há»£p vá»›i áº£nh hÆ°á»Ÿng thá»±c táº¿.
    
-   **PhÃ¹ há»£p cho nhiá»u kiáº¿n trÃºc máº¡ng sÃ¢u:**  tá»« máº¡ng Ä‘a lá»›p Ä‘Æ¡n giáº£n Ä‘áº¿n máº¡ng tÃ­ch cháº­p (CNN), máº¡ng há»“i quy (RNN)...

## 6.4. á»¨ng dá»¥ng Deep SHAP Ä‘á»ƒ giáº£i thÃ­ch mÃ´ hÃ¬nh dá»± bÃ¡o giÃ¡ cÄƒn há»™ táº¡i PhÃ¡p
Trong bÃ i viáº¿t nÃ y, chÃºng tÃ´i xÃ¢y dá»±ng mÃ´ hÃ¬nh dá»± bÃ¡o giÃ¡ cÄƒn há»™ dá»±a trÃªn dá»¯ liá»‡u lá»›n chá»©a hÆ¡n 1,5 triá»‡u giao dá»‹ch báº¥t Ä‘á»™ng sáº£n táº¡i PhÃ¡p. Dá»¯ liá»‡u gá»“m 62 biáº¿n Ä‘áº·c trÆ°ng mÃ´ táº£ chi tiáº¿t vá»‹ trÃ­, diá»‡n tÃ­ch, Ä‘áº·c Ä‘iá»ƒm dÃ¢n cÆ° khu vá»±c xung quanh.

MÃ´ hÃ¬nh sá»­ dá»¥ng lÃ  máº¡ng nÆ¡-ron Ä‘a lá»›p (deep neural network), gá»“m cÃ¡c lá»›p: 512 â†’ 256 â†’ 128 â†’ 64 â†’ 1 neuron cuá»‘i cÃ¹ng, káº¿t há»£p ká»¹ thuáº­t batch normalization vÃ  dropout Ä‘á»ƒ trÃ¡nh quÃ¡ khá»›p, giÃºp mÃ´ hÃ¬nh há»c cÃ¡c má»‘i quan há»‡ phá»©c táº¡p vÃ  phi tuyáº¿n tÃ­nh trong dá»¯ liá»‡u.

Sau khi huáº¥n luyá»‡n thÃ nh cÃ´ng, chÃºng tÃ´i tiáº¿n hÃ nh giáº£i thÃ­ch mÃ´ hÃ¬nh báº±ng phÆ°Æ¡ng phÃ¡p Deep SHAP Ä‘á»ƒ xÃ¡c Ä‘á»‹nh rÃµ tá»«ng biáº¿n áº£nh hÆ°á»Ÿng nhÆ° tháº¿ nÃ o Ä‘áº¿n dá»± bÃ¡o giÃ¡ cÄƒn há»™. CÃ¡c Ä‘áº·c trÆ°ng nhÆ° diá»‡n tÃ­ch xÃ¢y dá»±ng thá»±c táº¿, thu nháº­p cÆ° dÃ¢n trong khu vá»±c xung quanh, thÃ nh pháº§n nghá» nghiá»‡p dÃ¢n cÆ° Ä‘á»u Ä‘Æ°á»£c phÃ¢n tÃ­ch Ä‘á»ƒ hiá»ƒu rÃµ vai trÃ² vÃ  áº£nh hÆ°á»Ÿng cá»§a tá»«ng yáº¿u tá»‘ nÃ y Ä‘á»‘i vá»›i giÃ¡ trá»‹ cÄƒn há»™.

DÆ°á»›i Ä‘Ã¢y lÃ  vÃ­ dá»¥ minh há»a kÃ¨m káº¿t quáº£ cá»§a viá»‡c sá»­ dá»¥ng Deep SHAP Ä‘á»ƒ giáº£i thÃ­ch mÃ´ hÃ¬nh :

```python
def explain_model_deep_shap(
    model, 
    X_train_scaled, 
    X_test_scaled, 
    feature_cols, 
    device, 
    num_background=100, 
    num_samples=50, 
    feature_name_mapping=None,
    title_text="Model Explanation - SHAP Values"
):
    """
    Giáº£i thÃ­ch mÃ´ hÃ¬nh PyTorch báº±ng Deep SHAP.
    
    Args:
        model: PyTorch model Ä‘Ã£ huáº¥n luyá»‡n.
        X_train_scaled: Dá»¯ liá»‡u huáº¥n luyá»‡n Ä‘Ã£ chuáº©n hÃ³a.
        X_test_scaled: Dá»¯ liá»‡u kiá»ƒm tra Ä‘Ã£ chuáº©n hÃ³a.
        feature_cols: Danh sÃ¡ch tÃªn cÃ¡c biáº¿n Ä‘áº·c trÆ°ng.
        device: device dÃ¹ng Ä‘á»ƒ tÃ­nh toÃ¡n (cpu/cuda).
        num_background: sá»‘ máº«u dÃ¹ng lÃ m background cho SHAP.
        num_samples: sá»‘ máº«u Ä‘á»ƒ giáº£i thÃ­ch.
        feature_name_mapping: dict Ã¡nh xáº¡ tÃªn biáº¿n sang tÃªn hiá»ƒn thá»‹ (tÃ¹y chá»n).
        title_text: TiÃªu Ä‘á» tá»± do hiá»ƒn thá»‹ trÃªn biá»ƒu Ä‘á»“.
    """
    model.eval()
    
    background = torch.tensor(X_train_scaled[:num_background], dtype=torch.float32).to(device)
    explainer = shap.DeepExplainer(model, background)
    
    samples = torch.tensor(X_test_scaled[:num_samples], dtype=torch.float32).to(device)
    shap_values = explainer.shap_values(samples)
    
    # Chuyá»ƒn sang numpy array
    if isinstance(shap_values, list):
        shap_values_np = shap_values[0]
    else:
        shap_values_np = shap_values
        
    if isinstance(shap_values_np, torch.Tensor):
        shap_values_np = shap_values_np.cpu().detach().numpy()
    
    if shap_values_np.ndim == 3:
        shap_values_np = shap_values_np.squeeze(-1)
    
    # Chuyá»ƒn tÃªn biáº¿n náº¿u cÃ³ mapping
    if feature_name_mapping:
        feature_names_vn = [feature_name_mapping.get(col, col) for col in feature_cols]
    else:
        feature_names_vn = feature_cols
    
    # Cáº¥u hÃ¬nh font tiáº¿ng Viá»‡t, kÃ­ch thÆ°á»›c
    plt.rcParams['font.family'] = 'DejaVu Sans'
    plt.rcParams['font.size'] = 10
    
    # Váº½ biá»ƒu Ä‘á»“ SHAP
    shap.summary_plot(
        shap_values_np,
        X_test_scaled[:num_samples],
        feature_names=feature_names_vn,
        max_display=20,
        show=False
    )
    
    # ThÃªm tiÃªu Ä‘á» tá»± do phÃ­a trÃªn
    plt.gcf().text(
        0.5, 0.98, title_text, 
        fontsize=16, color='crimson', 
        fontweight='bold', ha='center', va='top'
    )
    plt.show()


if __name__ == "__main__":
    
    df = df_dummies[df_dummies['code_type_local'] == 2]

    df.drop(columns=['code_type_local' , 'prix_m2_iris'],inplace = True)
    
    print("DataFrame máº«u Ä‘Æ°á»£c táº¡o:")
    print(f"Shape: {df.shape}")
    print(f"Columns: {df.columns.tolist()[:5]}...\n")
    
    # Huáº¥n luyá»‡n mÃ´ hÃ¬nh
    model, scaler_X, scaler_y, X_train_scaled, X_test_scaled, feature_cols, device = \
        train_model_optim_gpu(df, epochs=300)
    
    # Giáº£i thÃ­ch mÃ´ hÃ¬nh báº±ng Deep SHAP
    shap_values = explain_model_deep_shap(
        model, 
        X_train_scaled, 
        X_test_scaled, 
        feature_cols,
        device,
        num_background=100,
        num_samples=50
    )
```

![Sá»­ dá»¥ng Deep SHAP Ä‘á»ƒ giáº£i thÃ­ch mÃ´ hÃ¬nh dá»± bÃ¡o giÃ¡ nhÃ ](images/deepshap.png)


# 7. Káº¿t luáº­n

Sá»± káº¿t há»£p giá»¯a **Genetic Algorithm (GA)** vÃ  **SHAP** má»Ÿ ra má»™t hÆ°á»›ng Ä‘i má»›i cho tá»‘i Æ°u hÃ³a cÃ³ thá»ƒ giáº£i thÃ­ch (**Explainable Optimization**) â€” nÆ¡i mÃ  mÃ´ hÃ¬nh khÃ´ng chá»‰ tÃ¬m ra káº¿t quáº£ tá»‘t nháº¥t, mÃ  cÃ²n cho chÃºng ta biáº¿t lÃ½ do vÃ¬ sao káº¿t quáº£ Ä‘Ã³ tá»‘t.

GA mang láº¡i kháº£ nÄƒng tÃ¬m kiáº¿m toÃ n cá»¥c máº¡nh máº½ trong nhá»¯ng khÃ´ng gian thiáº¿t káº¿ phá»©c táº¡p, phi tuyáº¿n vÃ  khÃ´ng kháº£ vi â€” Ä‘iá»u mÃ  cÃ¡c phÆ°Æ¡ng phÃ¡p cá»• Ä‘iá»ƒn nhÆ° Gradient Descent khÃ³ Ä‘áº¡t Ä‘Æ°á»£c. Trong khi Ä‘Ã³, SHAP vÃ  cÃ¡c biáº¿n thá»ƒ cá»§a nÃ³ nhÆ° Tree SHAP hay Kernel SHAP Ä‘Ã³ng vai trÃ² â€œgiáº£i pháº«uâ€ mÃ´ hÃ¬nh AI, giÃºp lÃ m rÃµ má»©c Ä‘á»™ áº£nh hÆ°á»Ÿng cá»§a tá»«ng yáº¿u tá»‘ Ä‘áº§u vÃ o Ä‘áº¿n káº¿t quáº£ tá»‘i Æ°u mÃ  GA tÃ¬m Ä‘Æ°á»£c. Nhá» Ä‘Ã³, há»‡ thá»‘ng GA + SHAP khÃ´ng chá»‰ tá»‘i Æ°u Ä‘Æ°á»£c thiáº¿t káº¿, cáº¥u hÃ¬nh hoáº·c chiáº¿n lÆ°á»£c, mÃ  cÃ²n cung cáº¥p cÃ¡i nhÃ¬n sÃ¢u sáº¯c, minh báº¡ch vÃ  Ä‘Ã¡ng tin cáº­y cho cÃ¡c nhÃ  ká»¹ sÆ°, nhÃ  khoa há»c vÃ  chuyÃªn gia ra quyáº¿t Ä‘á»‹nh.

Tá»« viá»‡c tá»‘i Æ°u khÃ­ Ä‘á»™ng há»c xe Ä‘ua, thiáº¿t káº¿ turbine giÃ³, lá»±a chá»n danh má»¥c Ä‘áº§u tÆ°, Ä‘áº¿n phÃ¡t triá»ƒn thuá»‘c vÃ  váº­t liá»‡u má»›i, sá»± káº¿t há»£p giá»¯a sá»©c máº¡nh tiáº¿n hÃ³a cá»§a GA vÃ  kháº£ nÄƒng giáº£i thÃ­ch cá»§a SHAP Ä‘ang trá»Ÿ thÃ nh xu hÆ°á»›ng táº¥t yáº¿u trong tháº¿ há»‡ AI ká»¹ thuáº­t vÃ  khoa há»c á»©ng dá»¥ng hiá»‡n Ä‘áº¡i â€” nÆ¡i hiá»‡u quáº£, hiá»ƒu biáº¿t vÃ  minh báº¡ch Ä‘i cÃ¹ng nhau.